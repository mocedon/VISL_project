{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Structure:\n",
        "\n",
        "- Get images\n",
        "  - Per patient\n",
        "  - Train\n",
        "  - Test\n",
        "-Run all images through feature detection\n",
        "-Take each patient, run PCA with k =?\n",
        "-Run MLP classifier / linear discriminator"
      ],
      "metadata": {
        "id": "hGa51nBR-3Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjYfD2Z1BMtV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data(dir = './data', ptns=12, pchs=10):\n",
        "  imgs = torch.rand((ptns, pchs,3, 224,224))\n",
        "  os.mkdir(dir)\n",
        "  classes = []\n",
        "  for p in range(ptns):\n",
        "    case = 'A' if torch.randn(1) > 0 else 'B'\n",
        "    classes.append(case)\n",
        "    p_name = \"ptn-{}-{}\".format(str(p).zfill(3), case)\n",
        "    pth = os.path.join(dir, p_name)\n",
        "    os.mkdir(pth)\n",
        "    for i in range(pchs):\n",
        "      im = imgs[int(p),i]\n",
        "      fname = os.path.join(dir ,p_name, str(i).zfill(3) + '.png')\n",
        "      torchvision.utils.save_image(im, fname)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "vT4M9f7I-e2u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr data*\n",
        "!rm -fr aug_data*\n",
        "!ls"
      ],
      "metadata": {
        "id": "KT2ybGgdSEBI",
        "outputId": "e5839a80-07d9-4caf-8e1e-d738a1ba3b2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "classes = fake_data(dir)\n",
        "!ls"
      ],
      "metadata": {
        "id": "HYnmUlwtB1JI",
        "outputId": "e216b7a4-d07f-48d9-b590-6a06d4061a5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(dir):\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(dev)\n",
        "  device = torch.device(dev)\n",
        "\n",
        "  default_transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToPILImage(), transforms.ToTensor()])\n",
        "  dataset = torchvision.datasets.DatasetFolder(root=dir, loader=torchvision.io.read_image, extensions='.png', transform=default_transform)\n",
        "  clc = []\n",
        "  for img, lbl in dataset:\n",
        "    clc.append(img)\n",
        "  print(\"clc: [{} , {}]\".format(len(clc),clc[0].shape))\n",
        "\n",
        "  tn = torch.Tensor(len(clc), *clc[0].shape)\n",
        "  print(tn.shape)\n",
        "  for i in range(len(clc)):\n",
        "    tn[i] = clc[i]\n",
        "  print(tn.shape)\n",
        "  tn.to(device)\n",
        "  mean = tn.mean(dim=(0,2,3)).cpu()\n",
        "  std = tn.std(dim=(0,2,3)).cpu()\n",
        "  print(\"mean = {}\".format(mean))\n",
        "  print(\"std = {}\".format(std))\n",
        "\n",
        "  normal_transform = transforms.Compose([default_transform,\n",
        "                                         transforms.Normalize(mean=mean, std=std)])\n",
        "  return normal_transform"
      ],
      "metadata": {
        "id": "OeJEgOYgMAhQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA_(imgs ,k=16):\n",
        "  print(\"batch input size: {}\".format(imgs.shape))\n",
        "  ret = imgs.mean(dim=0)\n",
        "  print(\"batch new size: {}\".format(ret.shape))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "wjgM5J7Ln2Fu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(ds, dir):\n",
        "  cls = ds.find_classes(dir)[0]\n",
        "  ret = []\n",
        "  for c in cls:\n",
        "    case = c[-1]\n",
        "    ret.append(case)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "zLBpwMzK0AC0"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA(imgs, fx, k=8):\n",
        "  bts = imgs.shape[0]\n",
        "  print(\"Batch PCA input size: {}\".format(imgs.shape))\n",
        "  ftrs = fx(imgs).reshape(bts, -1).cpu().numpy()\n",
        "  #ftr_var = (ftrs.T @ ftrs).cpu().numpy()\n",
        "  PCA_comp = PCA(k, svd_solver='full').fit(ftrs)\n",
        "  eig_vec = torch.Tensor(PCA_comp.components_)\n",
        "  print(\"Eigen Vector size :{} , with max: {}\".format(eig_vec.shape, eig_vec.max()))\n",
        "  return eig_vec"
      ],
      "metadata": {
        "id": "WQ5gJRu8-oF0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_img(imgs):\n",
        "  s = imgs.shape[-1] // 8\n",
        "  t = transforms.RandomCrop(s)\n",
        "  return t(imgs)\n"
      ],
      "metadata": {
        "id": "tULoXSwVAQpN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, head, rest):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encode = head\n",
        "    self.decode = reset\n",
        "  \n",
        "  def forward(self, imgs):\n",
        "    x = self.encode(imgs)\n",
        "    return self.decode(x)\n",
        "\n",
        "  def feature_ext(self, imgs):\n",
        "    return self.encode(imgs)"
      ],
      "metadata": {
        "id": "8ReMGN_XAxsF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_aug(dataset, aug_dir, class_fnc, aug_fnc, fx_fnc):\n",
        "\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(dev)\n",
        "  device = torch.device(dev)\n",
        "\n",
        "  classes = class_fnc(dataset, dir)\n",
        "\n",
        "  data = [[] for i in range(len(dataset.find_classes(dir)[0]))]\n",
        "  for img, lbl in dataset:\n",
        "    data[lbl].append(img)\n",
        "  print(\"data: [{} , {} , {}]\".format(len(data),len(data[0]), data[0][0].shape))\n",
        "\n",
        "  if not os.path.isdir(aug_dir):\n",
        "    os.mkdir(aug_dir)\n",
        "  \n",
        "  for p, d in enumerate(data):\n",
        "    tn = torch.Tensor(len(d), *d[0].shape)\n",
        "    for i in range(len(d)):\n",
        "      tn[i] = d[i]\n",
        "    tn.requires_grad_(False).to(device)\n",
        "\n",
        "    rd = aug_fnc(tn, fx_fnc).cpu()\n",
        "\n",
        "    curr_dir = os.path.join(aug_dir, classes[p])\n",
        "    if not os.path.isdir(curr_dir):\n",
        "      os.mkdir(curr_dir)\n",
        "    fname = os.path.join(curr_dir , str(p).zfill(3) + '.png')\n",
        "    torchvision.utils.save_image(rd, fname)"
      ],
      "metadata": {
        "id": "uc30a7QqwPc9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "aug_dir= './aug_data'\n",
        "\n",
        "transform = get_transform(dir)\n",
        "\n",
        "dataset = torchvision.datasets.DatasetFolder(root=dir, loader=torchvision.io.read_image, extensions='.png', transform=transform)\n",
        "\n",
        "batch_aug(dataset, aug_dir, get_class, batch_PCA, ds_img)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGNsd8h-TBtE",
        "outputId": "83bb47ee-a570-49be-bda6-5bbc480e8878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "clc: [120 , torch.Size([3, 224, 224])]\n",
            "torch.Size([120, 3, 224, 224])\n",
            "torch.Size([120, 3, 224, 224])\n",
            "mean = tensor([0.5001, 0.4999, 0.5001])\n",
            "std = tensor([0.2887, 0.2888, 0.2887])\n",
            "cpu\n",
            "data: [12 , 10 , torch.Size([3, 224, 224])]\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06858336180448532\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06946983188390732\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.07643172889947891\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.07119285315275192\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.07155369222164154\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06703756004571915\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.07311712950468063\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06898590177297592\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06955018639564514\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.0788973867893219\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.07258736342191696\n",
            "Batch PCA input size: torch.Size([10, 3, 224, 224])\n",
            "Eigen Vector size :torch.Size([8, 2352]) , with max: 0.06333442777395248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nhaZI7tInWWk"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hq76k7k4hyfy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CBbqkQ_-PRfa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class patches_DataSet(torchvision.datasets.DatasetFolder):\n",
        "  def __init__(root='./data'):\n",
        "    super(torchvision.datasets.DatasetFolder, seld).init__(root=root, loader=torchvision.io.read_image, extensions='.png')\n",
        "\n",
        "  def find_classes(dir):\n",
        "    return None"
      ],
      "metadata": {
        "id": "5aqZoNEuQTRi"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Project_jupyter.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}