{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Structure:\n",
        "\n",
        "- Get images\n",
        "  - Per patient\n",
        "  - Train\n",
        "  - Test\n",
        "-Run all images through feature detection\n",
        "-Take each patient, run PCA with k =?\n",
        "-Run MLP classifier / linear discriminator"
      ],
      "metadata": {
        "id": "hGa51nBR-3Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and drive"
      ],
      "metadata": {
        "id": "c-Gxaaa6oZeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import kornia\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import cv2\n",
        "\n",
        "import re\n",
        "import json \n",
        "import glob\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjYfD2Z1BMtV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive to save work\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Proj_B\n",
        "#!git clone https://github.com/mocedon/VISL_project\n",
        "#!git pull\n",
        "%cd VISL_project"
      ],
      "metadata": {
        "id": "xWZR02PBun-1",
        "outputId": "707e0f1a-1b62-4322-8950-c92d3de1101a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Proj_B\n",
            "/content/gdrive/MyDrive/Proj_B/VISL_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goofing around"
      ],
      "metadata": {
        "id": "SEAEhMJi8Ako"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1,2,3,4,5,6,7]\n",
        "l2 = l1\n",
        "l2 = [i for i in l2 if i%2 == 0]\n",
        "print(l1)\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "gCvaumsKVY_j",
        "outputId": "fc7d76e3-d2b5-4e7d-f3b9-1c01fd44b01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = [{\"t\":\"ts\" , \"m\":\"msi\", \"im\":0},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":1},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":2},\n",
        "     {\"t\":\"ts\" , \"m\":\"msi\", \"im\":3},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":4},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":5},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":6},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":7},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":8},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":9}]\n",
        "p = pd.DataFrame(d)\n",
        "p[(p[\"t\"]== 'ts')&(p[\"m\"]=='msi')]"
      ],
      "metadata": {
        "id": "R_2YFi638Dqs",
        "outputId": "1dad0306-2357-4f8e-e973-c000229cc6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fadc20fa-63ce-41f5-af99-46cff94549e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>m</th>\n",
              "      <th>im</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fadc20fa-63ce-41f5-af99-46cff94549e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    t    m  im\n",
              "0  ts  msi   0\n",
              "3  ts  msi   3"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(3,16,16)\n",
        "print(\"torch shape\", t.shape)\n",
        "print(\"np shape\", t.numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcHSoX8aFkj",
        "outputId": "3638d54f-a07f-4c49-e955-66718a8ec62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch shape torch.Size([3, 16, 16])\n",
            "np shape (3, 16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = None\n",
        "a = torch.Tensor([1,2,3])\n",
        "b = torch.Tensor([4,5,6])\n",
        "t = torch.vstack((t,a)) if t else a\n",
        "t = torch.vstack((t,b,a,b)) \n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3875yI9hfqz",
        "outputId": "15f25c1a-e0e3-448a-ca9b-94220829c953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake data"
      ],
      "metadata": {
        "id": "Mam2ZqAVoe-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data(dir = './data', ptns=12, pchs=10):\n",
        "  imgs = torch.rand((ptns, pchs,3, 224,224))\n",
        "  os.mkdir(dir)\n",
        "  classes = []\n",
        "  for p in range(ptns):\n",
        "    case = 'A' if torch.randn(1) > 0 else 'B'\n",
        "    classes.append(case)\n",
        "    p_name = \"ptn-{}-{}\".format(str(p).zfill(3), case)\n",
        "    pth = os.path.join(dir, p_name)\n",
        "    os.mkdir(pth)\n",
        "    for i in range(pchs):\n",
        "      im = imgs[int(p),i]\n",
        "      if case is 'B':\n",
        "        im = im / 2\n",
        "      fname = os.path.join(dir ,p_name, str(i).zfill(3) + '.png')\n",
        "      torchvision.utils.save_image(im, fname)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "vT4M9f7I-e2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr data*\n",
        "!rm -fr aug_data*\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2ybGgdSEBI",
        "outputId": "49454393-1f4f-4f8e-b4bb-42c421600e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "classes = fake_data(dir)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnmUlwtB1JI",
        "outputId": "4d0de4c2-2e6c-4760-c17a-539668d6c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA_(imgs ,k=16):\n",
        "  print(\"batch input size: {}\".format(imgs.shape))\n",
        "  ret = imgs.mean(dim=0)\n",
        "  print(\"batch new size: {}\".format(ret.shape))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "wjgM5J7Ln2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "tnWuDvFQAK9a",
        "outputId": "ed194e8a-123e-4243-b502-461f21f6d80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/Proj_B/VISL_project'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "h9IVl3Rdrk5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_img(imgs):\n",
        "  s = imgs.shape[-1] // 8\n",
        "  t = transforms.RandomCrop(s)\n",
        "  return t(imgs)"
      ],
      "metadata": {
        "id": "tULoXSwVAQpN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(dev)\n",
        "  return device"
      ],
      "metadata": {
        "id": "DmE9FA8Grg3R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_patches(fn ,id):\n",
        "  dir, bn = os.path.split(fn)\n",
        "  patient = '-'.join(bn.split('-')[3:5])\n",
        "  dir, label = os.path.split(dir)\n",
        "  dir, ds = os.path.split(dir)\n",
        "  return {'id': id,\n",
        "          'file_name': fn,\n",
        "          'set': ds,\n",
        "          'label': label,\n",
        "          'patient': patient}"
      ],
      "metadata": {
        "id": "PKUYbyo3Df3k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(dir_, dir):\n",
        "  \"\"\"Creeates directory if necessary and returns dir path\"\"\"\n",
        "  d = os.path.join(dir_,dir)\n",
        "  if not os.path.isdir(d):\n",
        "    os.mkdir(d)\n",
        "  return d"
      ],
      "metadata": {
        "id": "nKlyd25jVaan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_data(dir, nd, n):\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ['test', 'train']:\n",
        "    d = mkdir(nd, t)\n",
        "    for m in ['MSI', 'MSS']:\n",
        "      mkdir(d,m)\n",
        "      path = os.path.join(dir, t, m)\n",
        "      print(path)\n",
        "      for im in glob.glob(path + '/*.png')[:n]:\n",
        "        #print(im)\n",
        "        name = os.path.basename(dir)\n",
        "        repl = os.path.basename(nd)\n",
        "        np = im.replace(name, repl)\n",
        "        #print(np)\n",
        "        if not os.path.isfile(np):\n",
        "          shutil.copy(im, np)\n",
        "\n",
        "#ds_data('./../dataset', './../small_dataset', 10000)\n",
        "#ds_data('./../dataset', './../dataset_ds', 1000)"
      ],
      "metadata": {
        "id": "HAZO6Al6_Djz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation"
      ],
      "metadata": {
        "id": "9LnLAM_sq0rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DS Class"
      ],
      "metadata": {
        "id": "2XHqOXIVJhf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDataset(Dataset):\n",
        "  def __init__(self, dir, parse_fn=parse_patches, transform=None):\n",
        "    self.patch_list = []\n",
        "    for i, fn in enumerate(glob.glob(dir + \"/*/*/*.png\")):\n",
        "      self.patch_list.append(parse_fn(fn, i))\n",
        "    self.active_list = self.patch_list.copy()\n",
        "    self.patient_dict = None\n",
        "    self.transform = transform\n",
        "    print(\"found \"+str(len(self.patch_list)))\n",
        "\n",
        "  def set_filter(self, ds=None, lb=None, pt=None):\n",
        "    if (ds):\n",
        "      self.active_list = [p for p in self.active_list if p['set'] == ds]\n",
        "    if (lb):\n",
        "      self.active_list = [p for p in self.active_list if p['label'] == lb]\n",
        "    if (pt):\n",
        "      self.active_list = [p for p in self.active_list if p['patient'] == pt]\n",
        "  \n",
        "  def reset_filter(self):\n",
        "    self.active_list = self.patch_list.copy()\n",
        "  \n",
        "  def get_sct(self, sect, num=False):\n",
        "    l_ = [p[sect] for p in self.active_list]\n",
        "    l_dct = {}\n",
        "    for l in l_:\n",
        "      l_dct[l] = l_dct.get(l, 0) + 1\n",
        "    return l_dct.items() if num else l_dct.keys()\n",
        "\n",
        "  def get_patients(self, num=False):\n",
        "    return self.get_sct('patient', num)\n",
        "\n",
        "  def get_sets(self, num=False):\n",
        "    return self.get_sct('set', num)\n",
        "  \n",
        "  def get_labels(self, num=False):\n",
        "    return self.get_sct('label', num)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.active_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    fn = self.active_list[index][\"file_name\"]\n",
        "    lb = self.active_list[index]['label']\n",
        "    img = torchvision.io.read_image(fn)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return (img, lb)"
      ],
      "metadata": {
        "id": "xcISDAO3AGnw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension reduction "
      ],
      "metadata": {
        "id": "q7AfgojaKAQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_vec(mat, vec):\n",
        "  print(\"shape mat\", mat.shape)\n",
        "  print(\"shape vec\", vec.shape)\n",
        "  return mat\n",
        "\n",
        "\n",
        "class batch_PCA():\n",
        "  def __init__(self, k=2, mean=False, eig_val=False, fnc=None):\n",
        "    self.k = k\n",
        "    self.mean = mean\n",
        "    self.eig_val = eig_val\n",
        "    self.fnc = fnc\n",
        "\n",
        "  def __call__(self, ftrs):\n",
        "    if self.fnc is not None:\n",
        "      ftrs = self.fnc(ftrs)\n",
        "    #print(\"Batch PCA input size: {}\".format(ftrs.shape))\n",
        "    p, f = ftrs.shape\n",
        "    if p < f:\n",
        "      avg = torch.mean(ftrs, dim=0)\n",
        "      for i in range(int(p) - int(f)):\n",
        "        ftrs = torch.vstack((ftrs, avg))\n",
        "    ftrs = ftrs.detach().cpu().numpy()\n",
        "    #ftr_var = (ftrs.T @ ftrs).cpu().numpy()\n",
        "    PCA_comp = PCA(self.k, svd_solver='full').fit(ftrs)\n",
        "    eig_vec = torch.Tensor(PCA_comp.components_)\n",
        "    if self.mean:\n",
        "      mean = torch.mean(torch.from_numpy(ftrs), dim=0)\n",
        "      eig_vec = torch.vstack((eig_vec, mean))\n",
        "    if self.eig_val:\n",
        "      eig_val = torch.from_numpy(PCA_comp.singular_values_)\n",
        "      eig_vec = torch.vstack((eig_vec, eig_val))\n",
        "    #print(\"Eigen Vector size :{} , with max: {}\".format(eig_vec.shape, eig_vec.max()))\n",
        "    return eig_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ5gJRu8-oF0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def incpt3_out2():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def incpt3_out100():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.fc = model.fc[0]\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def patient_data(dir, nd, model_fnc, dr_fnc, trf=None):\n",
        "  model = model_fnc()\n",
        "  ds = PatchDataset(dir, transform=trf)\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ds.get_sets():\n",
        "    s = mkdir(nd, t)\n",
        "    for m in ds.get_labels():\n",
        "     d = mkdir(s, m)\n",
        "     ds.set_filter(ds=t, lb=m)\n",
        "     for p, sz in ds.get_patients(num=True):\n",
        "       print(\"Patient {} - of group {}-{} has {} patches\".format(p,t,m,sz))\n",
        "       ds.set_filter(ds=t, lb=m, pt=p)\n",
        "       fname = os.path.join(d, p + '.npy')\n",
        "       if (len(ds) >= 2) and not os.path.isfile(fname):\n",
        "        dr = dr_fnc(ds, model).cpu().numpy()\n",
        "        with open(fname, 'wb') as f:\n",
        "           np.save(f, dr)\n",
        "       ds.reset_filter()\n",
        "     ds.reset_filter()\n",
        "\n",
        "\n",
        "def patientPCA(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return batch_PCA(ftrs, k=2, mean=False, eig_val=False)\n",
        "\n",
        "\n",
        "def patientRes(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return ftrs\n",
        "\n",
        "\n",
        "def read_mtx(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    return np.load(f)"
      ],
      "metadata": {
        "id": "uc30a7QqwPc9"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Part"
      ],
      "metadata": {
        "id": "vBUT9hMun8Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPmodel(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(MLPmodel, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, input),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(input, 2), \n",
        "        nn.Softmax(dim=1))\n",
        "\n",
        "    \n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    return self.mlp(ev_)\n",
        "\n",
        "\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, 2, bias=True),\n",
        "        nn.Softmax(dim=1))\n",
        "\n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    ev_ = self.mlp(ev_)\n",
        "    return ev_"
      ],
      "metadata": {
        "id": "KRwgoPbuuZuI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Q_pswhil1ogM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, criterion, device):\n",
        "    \"\"\"Used on a set model to gain loss and accuracy, does not train\"\"\"\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader: # Batch wise check\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images) # Run the images through the net\n",
        "            _, predicted = torch.max(outputs.data, 1) # Top  result\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += criterion(outputs, labels).data.item() / len(dataloader)\n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, total_loss\n",
        "\n",
        "\n",
        "def train(hp):\n",
        "  \n",
        "  model = hp[\"model\"].to(\"cpu\")\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= hp['lr'])\n",
        "  ds_train, ds_test = hp['dataset']\n",
        "  ld_train = torch.utils.data.DataLoader(ds_train, batch_size=hp['bts'], shuffle=True)\n",
        "  ld_test = torch.utils.data.DataLoader(ds_test, batch_size=hp['bts'], shuffle=True)\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "    # training loop\n",
        "  train_error = []\n",
        "  test_error = []\n",
        "  for epoch in range(1, hp['epochs'] + 1):\n",
        "      model.train()  # put in training mode\n",
        "      running_loss = 0.0\n",
        "      epoch_time = time.time()\n",
        "      for i, data in enumerate(ld_train, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          # send them to device\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          #ref = torch.Tensor(1, 1)\n",
        "          #ref[0] = labels\n",
        "          #labels = ref\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)  # forward pass\n",
        "          loss = criterion(outputs, labels)  # calculate the loss\n",
        "          # always the same 3 steps\n",
        "          optimizer.zero_grad()  # zero the parameter gradients\n",
        "          loss.backward()  # backpropagation\n",
        "          optimizer.step()  # update parameters\n",
        "\n",
        "          # Add statistics\n",
        "          train_error.append(loss.data.item())\n",
        "          running_loss += loss.data.item()\n",
        "\n",
        "      # Advance in learning rate schedule\n",
        "      # Normalizing the loss by the total number of train batches\n",
        "      running_loss /= len(ld_train)\n",
        "\n",
        "      # Calculate training/test set accuracy of the existing model\n",
        "      train_accuracy, _= calculate_accuracy(model, ld_train, criterion, device)\n",
        "      test_accuracy, test_loss = calculate_accuracy(model, ld_test, criterion, device)\n",
        "      test_error.append(test_loss)\n",
        "      log = \"Epoch: {:2d} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "      #train_error.append(running_loss)\n",
        "      epoch_time = time.time() - epoch_time\n",
        "      log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "      print(log)\n",
        "    \n",
        "  print('==> Finished Training ...')\n",
        "\n",
        "  # Graph test and train loss across iterations\n",
        "  batches = len(ld_train)\n",
        "  itr = list(range(hp['epochs'] * batches))\n",
        "  plt.plot(itr, train_error, 'r--', label='Train batch loss', linewidth=0.5)\n",
        "  itr = list(range(batches,(hp['epochs'] * len(ld_train)+batches), batches))\n",
        "  plt.plot(itr, test_error, color='g', label='Test Epoch loss', linewidth= 3)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylim(0, max(train_error))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # save model\n",
        "  print('==> Saving model ...')\n",
        "  state = {\n",
        "      'net': model.state_dict(),\n",
        "      'epoch': epoch,\n",
        "  }\n",
        "  if not os.path.isdir('checkpoints'):\n",
        "      os.mkdir('checkpoints')\n",
        "  torch.save(state, './checkpoints/' + hp['name'] +'.pth')\n",
        "   "
      ],
      "metadata": {
        "id": "Wgw2EMQ3DBm_"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(dir_train, dir_test , model, name):\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  dataset = []\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_train, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_test, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "\n",
        "  hyper_parameters = {\n",
        "      'model': model,\n",
        "      'lr': 1e-3,\n",
        "      'bts': 1,\n",
        "      'dataset': dataset,\n",
        "      'epochs': 20,\n",
        "      'name': name\n",
        "  }\n",
        "  train(hyper_parameters)\n",
        "\n",
        "\n",
        "def exp_pca(ftrs):\n",
        "  return batch_PCA(torch.exp(ftrs))"
      ],
      "metadata": {
        "id": "DGNsd8h-TBtE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Networks"
      ],
      "metadata": {
        "id": "LdmB6u0loBwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data stats"
      ],
      "metadata": {
        "id": "3aaFm_f2l3k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class rgb2hsv():\n",
        "    \"\"\"Convert images from RGB to HSV.\"\"\"\n",
        "    def __call__(self, image):\n",
        "        # swap color axis because\n",
        "        # torch image: C x H x W\n",
        "        # numpy image: H x W x C\n",
        "        image = image.cpu().numpy()\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "        hsv = hsv.transpose((2, 0, 1))\n",
        "        hsv = torch.from_numpy(hsv)\n",
        "        return hsv\n",
        "\n",
        "\n",
        "def patch_stat(ds):\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=False)\n",
        "  hist = torch.zeros(3,256)\n",
        "  for im, _ in dl:\n",
        "    for i in range(256):\n",
        "      for c in range(3):\n",
        "        hist[c][i] += torch.sum(im[0][c] == i)\n",
        "  norm = torch.sum(hist[0][:])\n",
        "  hist = hist / norm\n",
        "  plt.plot(range(256), hist[0][:], color='r')\n",
        "  plt.plot(range(256), hist[1][:], color='g')\n",
        "  plt.plot(range(256), hist[2][:], color='b')\n",
        "  #plt.show()\n",
        "  hist /= 3\n",
        "  ent = (-hist) * torch.log(hist + 0.0001)\n",
        "  ent = torch.sum(ent)\n",
        "  print(\"Data entropy is : {}\".format(ent))\n",
        "\n"
      ],
      "metadata": {
        "id": "OeJEgOYgMAhQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "ds_hsv = PatchDataset('./../dataset_ds', transform=rgb2hsv())\n",
        "plt.subplot(321)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - all\")\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSI\")\n",
        "plt.subplot(323)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSI\")\n",
        "ds_hsv.reset_filter()\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSS\")\n",
        "plt.subplot(325)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSS\")\n",
        "\n",
        "\n",
        "ds_rgb = PatchDataset('./../dataset_ds')\n",
        "plt.subplot(322)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - all\")\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSI\")\n",
        "plt.subplot(324)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSI\")\n",
        "ds_rgb.reset_filter()\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSS\")\n",
        "plt.subplot(326)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSS\")"
      ],
      "metadata": {
        "id": "VkN0MKOcX3yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_res(dir):\n",
        "  ds = torchvision.datasets.DatasetFolder(root=dir, loader=read_mtx, extensions='.npy')\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  for i, data in enumerate(dl):\n",
        "    if i < 1:\n",
        "      mat, lb = data\n",
        "      lb= int(lb.cpu())\n",
        "      mat = mat.numpy()[0]\n",
        "      ln = len(mat[:,0])\n",
        "      cl = 0 if lb == 0 else 90\n",
        "      cl = [cl + int(10*l/ln) for l in range(ln)]\n",
        "      print(cl)\n",
        "      plt.scatter(mat[:,0], mat[:,1], c=cl)\n",
        "  plt.show()\n",
        "  \n",
        "show_res('../dataset_2d_matrix_mm')"
      ],
      "metadata": {
        "id": "0XOtJPuHyN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## runs"
      ],
      "metadata": {
        "id": "1xQLeTwFl60C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_2d_res', incpt3_out2, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "U6uLbdNpMFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_100d_res', incpt3_out100, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "5bjNG4Y1BTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_2d_res/train', '../dataset_2d_res/test', MLPmodel(8, batch_PCA(k=2, mean=True, eig_val=True, fnc=torch.exp)), \"LR_test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "gWPSlWJgAljl",
        "outputId": "9f18c384-78b3-479d-ef22-10ced6176082"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  1 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.78 secs\n",
            "Epoch:  2 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.78 secs\n",
            "Epoch:  3 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.95 secs\n",
            "Epoch:  4 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 2.08 secs\n",
            "Epoch:  5 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.90 secs\n",
            "Epoch:  6 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.90 secs\n",
            "Epoch:  7 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.88 secs\n",
            "Epoch:  8 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.88 secs\n",
            "Epoch:  9 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.84 secs\n",
            "Epoch: 10 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.84 secs\n",
            "Epoch: 11 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.84 secs\n",
            "Epoch: 12 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.77 secs\n",
            "Epoch: 13 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.77 secs\n",
            "Epoch: 14 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.80 secs\n",
            "Epoch: 15 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.80 secs\n",
            "Epoch: 16 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.77 secs\n",
            "Epoch: 17 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.77 secs\n",
            "Epoch: 18 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.78 secs\n",
            "Epoch: 19 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.81 secs\n",
            "Epoch: 20 | Loss: nan | Training accuracy: 15.000% | Test accuracy: 26.000% | Epoch Time: 1.76 secs\n",
            "==> Finished Training ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Saving model ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_100d_res/train', '../dataset_100d_res/test', LinearReg(200, exp_pca), \"100d_pca\")"
      ],
      "metadata": {
        "id": "zQgkqMQmmR5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_100d_res/train', '../dataset_100d_res/test', MLPmodel(200, exp_pca), \"100d_pca\")"
      ],
      "metadata": {
        "id": "9VHqTfgW2CjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grave yard"
      ],
      "metadata": {
        "id": "xg44H4J-Um71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class vgg16_features(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg16_features, self).__init__()\n",
        "    self.m = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "  def forward(self, imgs):\n",
        "    ftrs = self.m.features(imgs)\n",
        "    return torch.max(ftrs, dim=1).values\n",
        "    "
      ],
      "metadata": {
        "id": "La5H9qOey8Vs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, head, rest):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encode = head\n",
        "    self.decode = rest\n",
        "  \n",
        "  def forward(self, imgs):\n",
        "    x = self.encode(imgs)\n",
        "    return self.decode(x)\n",
        "\n",
        "  def feature_ext(self, imgs):\n",
        "    return self.encode(imgs)"
      ],
      "metadata": {
        "id": "8ReMGN_XAxsF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class patches_DataSet(torchvision.datasets.DatasetFolder):\n",
        "  def __init__(self, root='./data'):\n",
        "    super(torchvision.datasets.DatasetFolder, self).init__(root=root, loader=torchvision.io.read_image, extensions='.png')\n",
        "\n",
        "  def find_classes(dir):\n",
        "    return None"
      ],
      "metadata": {
        "id": "5aqZoNEuQTRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(ds, dir):\n",
        "  cls = ds.find_classes(dir)[0]\n",
        "  ret = []\n",
        "  for c in cls:\n",
        "    case = c[-1]\n",
        "    ret.append(case)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "zLBpwMzK0AC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Project_jupyter.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}