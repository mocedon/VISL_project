{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Structure:\n",
        "\n",
        "- Get images\n",
        "  - Per patient\n",
        "  - Train\n",
        "  - Test\n",
        "-Run all images through feature detection\n",
        "-Take each patient, run PCA with k =?\n",
        "-Run MLP classifier / linear discriminator"
      ],
      "metadata": {
        "id": "hGa51nBR-3Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and drive"
      ],
      "metadata": {
        "id": "c-Gxaaa6oZeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import re\n",
        "import json \n",
        "import glob\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjYfD2Z1BMtV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive to save work\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Proj_B\n",
        "!git clone https://github.com/mocedon/VISL_project\n",
        "!git pull\n",
        "%cd VISL_project"
      ],
      "metadata": {
        "id": "2rssjW2KoqrM",
        "outputId": "97c21c8b-1ea4-432c-ea61-1803dc172cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Proj_B\n",
            "Cloning into 'VISL_project'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 26 (delta 5), reused 3 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (26/26), done.\n",
            "fatal: not a git repository (or any parent up to mount point /content)\n",
            "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
            "/content/gdrive/MyDrive/Proj_B/VISL_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goofing around"
      ],
      "metadata": {
        "id": "SEAEhMJi8Ako"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1,2,3,4,5,6,7]\n",
        "l2 = l1\n",
        "l2 = [i for i in l2 if i%2 == 0]\n",
        "print(l1)\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "gCvaumsKVY_j",
        "outputId": "fc7d76e3-d2b5-4e7d-f3b9-1c01fd44b01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = [{\"t\":\"ts\" , \"m\":\"msi\", \"im\":0},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":1},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":2},\n",
        "     {\"t\":\"ts\" , \"m\":\"msi\", \"im\":3},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":4},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":5},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":6},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":7},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":8},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":9}]\n",
        "p = pd.DataFrame(d)\n",
        "p[(p[\"t\"]== 'ts')&(p[\"m\"]=='msi')]"
      ],
      "metadata": {
        "id": "R_2YFi638Dqs",
        "outputId": "1dad0306-2357-4f8e-e973-c000229cc6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fadc20fa-63ce-41f5-af99-46cff94549e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>m</th>\n",
              "      <th>im</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fadc20fa-63ce-41f5-af99-46cff94549e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    t    m  im\n",
              "0  ts  msi   0\n",
              "3  ts  msi   3"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake data"
      ],
      "metadata": {
        "id": "Mam2ZqAVoe-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data(dir = './data', ptns=12, pchs=10):\n",
        "  imgs = torch.rand((ptns, pchs,3, 224,224))\n",
        "  os.mkdir(dir)\n",
        "  classes = []\n",
        "  for p in range(ptns):\n",
        "    case = 'A' if torch.randn(1) > 0 else 'B'\n",
        "    classes.append(case)\n",
        "    p_name = \"ptn-{}-{}\".format(str(p).zfill(3), case)\n",
        "    pth = os.path.join(dir, p_name)\n",
        "    os.mkdir(pth)\n",
        "    for i in range(pchs):\n",
        "      im = imgs[int(p),i]\n",
        "      if case is 'B':\n",
        "        im = im / 2\n",
        "      fname = os.path.join(dir ,p_name, str(i).zfill(3) + '.png')\n",
        "      torchvision.utils.save_image(im, fname)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "vT4M9f7I-e2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr data*\n",
        "!rm -fr aug_data*\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2ybGgdSEBI",
        "outputId": "49454393-1f4f-4f8e-b4bb-42c421600e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "classes = fake_data(dir)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnmUlwtB1JI",
        "outputId": "4d0de4c2-2e6c-4760-c17a-539668d6c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA_(imgs ,k=16):\n",
        "  print(\"batch input size: {}\".format(imgs.shape))\n",
        "  ret = imgs.mean(dim=0)\n",
        "  print(\"batch new size: {}\".format(ret.shape))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "wjgM5J7Ln2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "h9IVl3Rdrk5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_img(imgs):\n",
        "  s = imgs.shape[-1] // 8\n",
        "  t = transforms.RandomCrop(s)\n",
        "  return t(imgs)"
      ],
      "metadata": {
        "id": "tULoXSwVAQpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(dev)\n",
        "  return device"
      ],
      "metadata": {
        "id": "DmE9FA8Grg3R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_patches(fn ,id):\n",
        "  dir, bn = os.path.split(fn)\n",
        "  patient = '-'.join(bn.split('-')[3:5])\n",
        "  dir, label = os.path.split(dir)\n",
        "  dir, ds = os.path.split(dir)\n",
        "  return {'id': id,\n",
        "          'file_name': fn,\n",
        "          'set': ds,\n",
        "          'label': label,\n",
        "          'patient': patient}"
      ],
      "metadata": {
        "id": "PKUYbyo3Df3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(dir_, dir):\n",
        "  \"\"\"Creeates directory if necessary and returns dir path\"\"\"\n",
        "  d = os.path.join(dir_,dir)\n",
        "  if not os.path.isdir(d):\n",
        "    os.mkdir(d)\n",
        "  return d"
      ],
      "metadata": {
        "id": "nKlyd25jVaan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation"
      ],
      "metadata": {
        "id": "9LnLAM_sq0rC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDataset(Dataset):\n",
        "  def __init__(self, dir, parse_fn, transform):\n",
        "    self.patch_list = []\n",
        "    for fn in glob.glob(dir + \"/**/*.png\"):\n",
        "      self.patch_list.append(parse_fn(fn, len(self.patch_list)))\n",
        "    self.active_list = self.patch_list.copy()\n",
        "    self.patient_dict = None\n",
        "    self.transform = transform\n",
        "\n",
        "  def set_filter(self, ds=None, lb=None, pt=None):\n",
        "    if (ds):\n",
        "      self.active_list = [p for p in self.active_list if p['set'] == ds]\n",
        "    if (lb):\n",
        "      self.active_list = [p for p in self.active_list if p['label'] == lb]\n",
        "    if (pt):\n",
        "      self.active_list = [p for p in self.active_list if p['patient'] == pt]\n",
        "  \n",
        "  def reset_filter(self):\n",
        "    self.active_list = self.patch_list.copy()\n",
        "  \n",
        "  def get_patients(self):\n",
        "    pl = [p['patient'] for p in self.patch_list]\n",
        "    pl_dct = {}\n",
        "    for p in pl:\n",
        "      pl_dct[p] = pl_dct.get(p, 0) + 1\n",
        "    self.patient_dict = pl_dct\n",
        "    return pl_dct.keys()\n",
        "\n",
        "  def patient_patch_num(self, pt):\n",
        "    return self.patient_dict[pt]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.active_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    fn = self.active_list[index][\"file_name\"]\n",
        "    lb = self.active_list[index]['label']\n",
        "    img = torchvision.io.read_image(fn)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return (img, lb)"
      ],
      "metadata": {
        "id": "xcISDAO3AGnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def (ds):\n",
        "  default_transform = transforms.Compose([transforms.CenterCrop(224), transforms.ToPILImage(), transforms.ToTensor()])\n",
        "  dataset = torchvision.datasets.DatasetFolder(root=dir, loader=torchvision.io.read_image, extensions='.png', transform=default_transform)\n",
        "  clc = []\n",
        "  for img, lbl in dataset:\n",
        "    clc.append(img)\n",
        "  print(\"clc: [{} , {}]\".format(len(clc),clc[0].shape))\n",
        "\n",
        "  tn = torch.Tensor(len(clc), *clc[0].shape)\n",
        "  print(tn.shape)\n",
        "  for i in range(len(clc)):\n",
        "    tn[i] = clc[i]\n",
        "  print(tn.shape)\n",
        "  tn.to(get_device())\n",
        "  dim = [i for i in range(2, len(tn.shape))]\n",
        "  print('dim is : {}'.format(dim))\n",
        "  mean = tn.mean(dim=(0,*dim)).cpu()\n",
        "  std = tn.std(dim=(0,2,3)).cpu()\n",
        "  print(\"mean = {}\".format(mean))\n",
        "  print(\"std = {}\".format(std))\n",
        "\n",
        "  normal_transform = transforms.Compose([default_transform,\n",
        "                                         transforms.Normalize(mean=mean, std=std)])\n",
        "  return normal_transform"
      ],
      "metadata": {
        "id": "OeJEgOYgMAhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA(imgs, fx, k=8):\n",
        "  bts = imgs.shape[0]\n",
        "  print(\"Batch PCA input size: {}\".format(imgs.shape))\n",
        "  ftrs = fx(imgs).reshape(bts, -1).detach().cpu().numpy()\n",
        "  #ftr_var = (ftrs.T @ ftrs).cpu().numpy()\n",
        "  PCA_comp = PCA(k, svd_solver='full').fit(ftrs)\n",
        "  eig_vec = torch.Tensor(PCA_comp.components_)\n",
        "  print(\"Eigen Vector size :{} , with max: {}\".format(eig_vec.shape, eig_vec.max()))\n",
        "  return eig_vec"
      ],
      "metadata": {
        "id": "WQ5gJRu8-oF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_aug(dataset, aug_dir, class_fnc, aug_fnc, fx_fnc):\n",
        "  classes = class_fnc(dataset, dir)\n",
        "\n",
        "  data = [[] for i in range(len(dataset.find_classes(dir)[0]))]\n",
        "  for img, lbl in dataset:\n",
        "    data[lbl].append(img)\n",
        "  print(\"data: [{} , {} , {}]\".format(len(data),len(data[0]), data[0][0].shape))\n",
        "\n",
        "  if not os.path.isdir(aug_dir):\n",
        "    os.mkdir(aug_dir)\n",
        "  \n",
        "  for p, d in enumerate(data):\n",
        "    tn = torch.Tensor(len(d), *d[0].shape)\n",
        "    for i in range(len(d)):\n",
        "      tn[i] = d[i]\n",
        "    tn.requires_grad_(False).to(get_device())\n",
        "\n",
        "    rd = aug_fnc(tn, fx_fnc).cpu().numpy()\n",
        "\n",
        "    curr_dir = os.path.join(aug_dir, classes[p])\n",
        "    if not os.path.isdir(curr_dir):\n",
        "      os.mkdir(curr_dir)\n",
        "    fname = os.path.join(curr_dir , str(p).zfill(3) + '.npy')\n",
        "    with open(fname, 'wb') as f:\n",
        "      np.save(f, rd)\n",
        "\n",
        "\n",
        "def read_mtx(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    return np.load(f)"
      ],
      "metadata": {
        "id": "uc30a7QqwPc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Part"
      ],
      "metadata": {
        "id": "vBUT9hMun8Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, head, rest):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encode = head\n",
        "    self.decode = rest\n",
        "  \n",
        "  def forward(self, imgs):\n",
        "    x = self.encode(imgs)\n",
        "    return self.decode(x)\n",
        "\n",
        "  def feature_ext(self, imgs):\n",
        "    return self.encode(imgs)"
      ],
      "metadata": {
        "id": "8ReMGN_XAxsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class vgg16_features(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg16_features, self).__init__()\n",
        "    self.m = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "  def forward(self, imgs):\n",
        "    ftrs = self.m.features(imgs)\n",
        "    return torch.max(ftrs, dim=1).values\n",
        "    "
      ],
      "metadata": {
        "id": "La5H9qOey8Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPmodel(nn.Module):\n",
        "  def __init__(self, input):\n",
        "    super(MLPmodel, self).__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, input//8),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(input//8, 2), \n",
        "        nn.Softmax(dim=0))\n",
        "\n",
        "    \n",
        "  def forward(self, ev):\n",
        "    ev_ = ev.view(ev.shape[0], -1)\n",
        "    return self.mlp(ev_)\n",
        "\n",
        "\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, 2, bias=True),\n",
        "        nn.Softmax())\n",
        "\n",
        "  def forward(self, ev):\n",
        "    ev_ = ev.view(ev.shape[0], -1)\n",
        "    ev_ = self.mlp(ev_)\n",
        "\n",
        "    return ev_\n",
        "\n",
        "\n",
        "\n",
        "def calculate_accuracy(model, dataloader, criterion, device):\n",
        "    \"\"\"Used on a set model to gain loss and accuracy, does not train\"\"\"\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader: # Batch wise check\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images) # Run the images through the net\n",
        "            _, predicted = torch.max(outputs.data, 1) # Top  result\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += criterion(outputs, labels).data.item() / len(dataloader)\n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, total_loss\n",
        "\n",
        "\n",
        "def train(hp):\n",
        "  \n",
        "  model = hp[\"model\"].to(get_device())\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= hp['lr'])\n",
        "  ds_train, ds_test = hp['dataset']\n",
        "  ld_train = torch.utils.data.DataLoader(ds_train, batch_size=hp['bts'], shuffle=True)\n",
        "  ld_test = torch.utils.data.DataLoader(ds_test, batch_size=hp['bts'], shuffle=True)\n",
        "  device = get_device()\n",
        "\n",
        "    # training loop\n",
        "  train_error = []\n",
        "  test_error = []\n",
        "  for epoch in range(1, hp['epochs'] + 1):\n",
        "      model.train()  # put in training mode\n",
        "      running_loss = 0.0\n",
        "      epoch_time = time.time()\n",
        "      for i, data in enumerate(ld_train, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          # send them to device\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)  # forward pass\n",
        "          loss = criterion(outputs, labels)  # calculate the loss\n",
        "          # always the same 3 steps\n",
        "          optimizer.zero_grad()  # zero the parameter gradients\n",
        "          loss.backward()  # backpropagation\n",
        "          optimizer.step()  # update parameters\n",
        "\n",
        "          # Add statistics\n",
        "          train_error.append(loss.data.item())\n",
        "          running_loss += loss.data.item()\n",
        "\n",
        "      # Advance in learning rate schedule\n",
        "      # Normalizing the loss by the total number of train batches\n",
        "      running_loss /= len(ld_train)\n",
        "\n",
        "      # Calculate training/test set accuracy of the existing model\n",
        "      train_accuracy, _= calculate_accuracy(model, ld_train, criterion, device)\n",
        "      test_accuracy, test_loss = calculate_accuracy(model, ld_test, criterion, device)\n",
        "      test_error.append(test_loss)\n",
        "      log = \"Epoch: {:2d} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "      #train_error.append(running_loss)\n",
        "      epoch_time = time.time() - epoch_time\n",
        "      log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "      print(log)\n",
        "    \n",
        "  print('==> Finished Training ...')\n",
        "\n",
        "  # Graph test and train loss across iterations\n",
        "  batches = len(ld_train)\n",
        "  itr = list(range(hp['epochs'] * batches))\n",
        "  plt.plot(itr, train_error, 'r--', label='Train batch loss', linewidth=0.5)\n",
        "  itr = list(range(batches,(hp['epochs'] * len(ld_train)+batches), batches))\n",
        "  plt.plot(itr, test_error, color='g', label='Test Epoch loss', linewidth= 3)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylim(0, max(train_error))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # save model\n",
        "  print('==> Saving model ...')\n",
        "  state = {\n",
        "      'net': model.state_dict(),\n",
        "      'epoch': epoch,\n",
        "  }\n",
        "  if not os.path.isdir('checkpoints'):\n",
        "      os.mkdir('checkpoints')\n",
        "  torch.save(state, './checkpoints/' + hp['name'] +'.pth')\n",
        "   "
      ],
      "metadata": {
        "id": "Wgw2EMQ3DBm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Networks"
      ],
      "metadata": {
        "id": "LdmB6u0loBwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(dir, dir_new):\n",
        "  model = vgg16_features()\n",
        "  model.eval()\n",
        "  #model.features()\n",
        "  transform = get_transform(dir)\n",
        "  dataset = torchvision.datasets.DatasetFolder(root=dir, loader=torchvision.io.read_image, extensions='.png', transform=transform)\n",
        "  batch_aug(dataset, dir_new, get_class, batch_PCA, model)\n",
        "\n",
        "def process(dir_train, dir_test , model, name):\n",
        "  transform = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
        "  dataset = []\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_train, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_test, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "\n",
        "  hyper_parameters = {\n",
        "      'model': model,\n",
        "      'lr': 1e-3,\n",
        "      'bts': 2,\n",
        "      'dataset': dataset,\n",
        "      'epochs': 12,\n",
        "      'name': name\n",
        "  }\n",
        "  train(hyper_parameters)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DGNsd8h-TBtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "aug_dir= './aug_data'\n",
        "preprocess(dir, aug_dir)\n"
      ],
      "metadata": {
        "id": "nhaZI7tInWWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process(aug_dir, aug_dir, MLPmodel(392), \"MLP_test\")\n",
        "process(aug_dir, aug_dir, LinearReg(392), \"LR_test\")"
      ],
      "metadata": {
        "id": "hq76k7k4hyfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grave yard"
      ],
      "metadata": {
        "id": "xg44H4J-Um71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class patches_DataSet(torchvision.datasets.DatasetFolder):\n",
        "  def __init__(root='./data'):\n",
        "    super(torchvision.datasets.DatasetFolder, seld).init__(root=root, loader=torchvision.io.read_image, extensions='.png')\n",
        "\n",
        "  def find_classes(dir):\n",
        "    return None"
      ],
      "metadata": {
        "id": "5aqZoNEuQTRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(ds, dir):\n",
        "  cls = ds.find_classes(dir)[0]\n",
        "  ret = []\n",
        "  for c in cls:\n",
        "    case = c[-1]\n",
        "    ret.append(case)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "zLBpwMzK0AC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Project_jupyter.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}