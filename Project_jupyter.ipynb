{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Structure:\n",
        "\n",
        "- Get images\n",
        "  - Per patient\n",
        "  - Train\n",
        "  - Test\n",
        "-Run all images through feature detection\n",
        "-Take each patient, run PCA with k =?\n",
        "-Run MLP classifier / linear discriminator"
      ],
      "metadata": {
        "id": "hGa51nBR-3Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and drive"
      ],
      "metadata": {
        "id": "c-Gxaaa6oZeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import kornia\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import cv2\n",
        "\n",
        "import re\n",
        "import json \n",
        "import glob\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjYfD2Z1BMtV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive to save work\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Proj_B\n",
        "#!git clone https://github.com/mocedon/VISL_project\n",
        "#!git pull\n",
        "%cd VISL_project"
      ],
      "metadata": {
        "id": "xWZR02PBun-1",
        "outputId": "3e7ce22d-e9d0-4174-a3c6-16e996841274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Proj_B\n",
            "/content/gdrive/MyDrive/Proj_B/VISL_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goofing around"
      ],
      "metadata": {
        "id": "SEAEhMJi8Ako"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1,2,3,4,5,6,7]\n",
        "l2 = l1\n",
        "l2 = [i for i in l2 if i%2 == 0]\n",
        "print(l1)\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "gCvaumsKVY_j",
        "outputId": "fc7d76e3-d2b5-4e7d-f3b9-1c01fd44b01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = [{\"t\":\"ts\" , \"m\":\"msi\", \"im\":0},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":1},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":2},\n",
        "     {\"t\":\"ts\" , \"m\":\"msi\", \"im\":3},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":4},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":5},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":6},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":7},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":8},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":9}]\n",
        "p = pd.DataFrame(d)\n",
        "p[(p[\"t\"]== 'ts')&(p[\"m\"]=='msi')]"
      ],
      "metadata": {
        "id": "R_2YFi638Dqs",
        "outputId": "1dad0306-2357-4f8e-e973-c000229cc6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fadc20fa-63ce-41f5-af99-46cff94549e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>m</th>\n",
              "      <th>im</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fadc20fa-63ce-41f5-af99-46cff94549e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    t    m  im\n",
              "0  ts  msi   0\n",
              "3  ts  msi   3"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(3,16,16)\n",
        "print(\"torch shape\", t.shape)\n",
        "print(\"np shape\", t.numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcHSoX8aFkj",
        "outputId": "3638d54f-a07f-4c49-e955-66718a8ec62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch shape torch.Size([3, 16, 16])\n",
            "np shape (3, 16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = None\n",
        "a = torch.Tensor([1,2,3])\n",
        "b = torch.Tensor([4,5,6])\n",
        "t = torch.vstack((t,a)) if t else a\n",
        "t = torch.vstack((t,b,a,b)) \n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3875yI9hfqz",
        "outputId": "15f25c1a-e0e3-448a-ca9b-94220829c953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake data"
      ],
      "metadata": {
        "id": "Mam2ZqAVoe-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data(dir = './data', ptns=12, pchs=10):\n",
        "  imgs = torch.rand((ptns, pchs,3, 224,224))\n",
        "  os.mkdir(dir)\n",
        "  classes = []\n",
        "  for p in range(ptns):\n",
        "    case = 'A' if torch.randn(1) > 0 else 'B'\n",
        "    classes.append(case)\n",
        "    p_name = \"ptn-{}-{}\".format(str(p).zfill(3), case)\n",
        "    pth = os.path.join(dir, p_name)\n",
        "    os.mkdir(pth)\n",
        "    for i in range(pchs):\n",
        "      im = imgs[int(p),i]\n",
        "      if case is 'B':\n",
        "        im = im / 2\n",
        "      fname = os.path.join(dir ,p_name, str(i).zfill(3) + '.png')\n",
        "      torchvision.utils.save_image(im, fname)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "vT4M9f7I-e2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr data*\n",
        "!rm -fr aug_data*\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2ybGgdSEBI",
        "outputId": "49454393-1f4f-4f8e-b4bb-42c421600e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "classes = fake_data(dir)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnmUlwtB1JI",
        "outputId": "4d0de4c2-2e6c-4760-c17a-539668d6c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA_(imgs ,k=16):\n",
        "  print(\"batch input size: {}\".format(imgs.shape))\n",
        "  ret = imgs.mean(dim=0)\n",
        "  print(\"batch new size: {}\".format(ret.shape))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "wjgM5J7Ln2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "tnWuDvFQAK9a",
        "outputId": "ed194e8a-123e-4243-b502-461f21f6d80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/Proj_B/VISL_project'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "h9IVl3Rdrk5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_img(imgs):\n",
        "  s = imgs.shape[-1] // 8\n",
        "  t = transforms.RandomCrop(s)\n",
        "  return t(imgs)"
      ],
      "metadata": {
        "id": "tULoXSwVAQpN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(dev)\n",
        "  return device"
      ],
      "metadata": {
        "id": "DmE9FA8Grg3R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_patches(fn ,id):\n",
        "  dir, bn = os.path.split(fn)\n",
        "  patient = '-'.join(bn.split('-')[3:5])\n",
        "  dir, label = os.path.split(dir)\n",
        "  dir, ds = os.path.split(dir)\n",
        "  return {'id': id,\n",
        "          'file_name': fn,\n",
        "          'set': ds,\n",
        "          'label': label,\n",
        "          'patient': patient}"
      ],
      "metadata": {
        "id": "PKUYbyo3Df3k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(dir_, dir):\n",
        "  \"\"\"Creeates directory if necessary and returns dir path\"\"\"\n",
        "  d = os.path.join(dir_,dir)\n",
        "  if not os.path.isdir(d):\n",
        "    os.mkdir(d)\n",
        "  return d"
      ],
      "metadata": {
        "id": "nKlyd25jVaan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_data(dir, nd, n):\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ['test', 'train']:\n",
        "    d = mkdir(nd, t)\n",
        "    for m in ['MSI', 'MSS']:\n",
        "      mkdir(d,m)\n",
        "      path = os.path.join(dir, t, m)\n",
        "      print(path)\n",
        "      for im in glob.glob(path + '/*.png')[:n]:\n",
        "        #print(im)\n",
        "        name = os.path.basename(dir)\n",
        "        repl = os.path.basename(nd)\n",
        "        np = im.replace(name, repl)\n",
        "        #print(np)\n",
        "        if not os.path.isfile(np):\n",
        "          shutil.copy(im, np)\n",
        "\n",
        "#ds_data('./../dataset', './../small_dataset', 10000)\n",
        "#ds_data('./../dataset', './../dataset_ds', 1000)"
      ],
      "metadata": {
        "id": "HAZO6Al6_Djz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation"
      ],
      "metadata": {
        "id": "9LnLAM_sq0rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DS Class"
      ],
      "metadata": {
        "id": "2XHqOXIVJhf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDataset(Dataset):\n",
        "  def __init__(self, dir, parse_fn=parse_patches, transform=None):\n",
        "    self.patch_list = []\n",
        "    for i, fn in enumerate(glob.glob(dir + \"/*/*/*.png\")):\n",
        "      self.patch_list.append(parse_fn(fn, i))\n",
        "    self.active_list = self.patch_list.copy()\n",
        "    self.patient_dict = None\n",
        "    self.transform = transform\n",
        "    print(\"found \"+str(len(self.patch_list)))\n",
        "\n",
        "  def set_filter(self, ds=None, lb=None, pt=None):\n",
        "    if (ds):\n",
        "      self.active_list = [p for p in self.active_list if p['set'] == ds]\n",
        "    if (lb):\n",
        "      self.active_list = [p for p in self.active_list if p['label'] == lb]\n",
        "    if (pt):\n",
        "      self.active_list = [p for p in self.active_list if p['patient'] == pt]\n",
        "  \n",
        "  def reset_filter(self):\n",
        "    self.active_list = self.patch_list.copy()\n",
        "  \n",
        "  def get_sct(self, sect, num=False):\n",
        "    l_ = [p[sect] for p in self.active_list]\n",
        "    l_dct = {}\n",
        "    for l in l_:\n",
        "      l_dct[l] = l_dct.get(l, 0) + 1\n",
        "    return l_dct.items() if num else l_dct.keys()\n",
        "\n",
        "  def get_patients(self, num=False):\n",
        "    return self.get_sct('patient', num)\n",
        "\n",
        "  def get_sets(self, num=False):\n",
        "    return self.get_sct('set', num)\n",
        "  \n",
        "  def get_labels(self, num=False):\n",
        "    return self.get_sct('label', num)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.active_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    fn = self.active_list[index][\"file_name\"]\n",
        "    lb = self.active_list[index]['label']\n",
        "    img = torchvision.io.read_image(fn)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return (img, lb)"
      ],
      "metadata": {
        "id": "xcISDAO3AGnw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data stat"
      ],
      "metadata": {
        "id": "tKwF_a_TJkk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class rgb2hsv():\n",
        "    \"\"\"Convert images from RGB to HSV.\"\"\"\n",
        "    def __call__(self, image):\n",
        "        # swap color axis because\n",
        "        # torch image: C x H x W\n",
        "        # numpy image: H x W x C\n",
        "        image = image.cpu().numpy()\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "        hsv = hsv.transpose((2, 0, 1))\n",
        "        hsv = torch.from_numpy(hsv)\n",
        "        return hsv\n",
        "\n",
        "\n",
        "def patch_stat(ds):\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=False)\n",
        "  hist = torch.zeros(3,256)\n",
        "  for im, _ in dl:\n",
        "    for i in range(256):\n",
        "      for c in range(3):\n",
        "        hist[c][i] += torch.sum(im[0][c] == i)\n",
        "  norm = torch.sum(hist[0][:])\n",
        "  hist = hist / norm\n",
        "  plt.plot(range(256), hist[0][:], color='r')\n",
        "  plt.plot(range(256), hist[1][:], color='g')\n",
        "  plt.plot(range(256), hist[2][:], color='b')\n",
        "  #plt.show()\n",
        "  hist /= 3\n",
        "  ent = (-hist) * torch.log(hist + 0.0001)\n",
        "  ent = torch.sum(ent)\n",
        "  print(\"Data entropy is : {}\".format(ent))\n",
        "\n"
      ],
      "metadata": {
        "id": "OeJEgOYgMAhQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "ds_hsv = PatchDataset('./../dataset_ds', transform=rgb2hsv())\n",
        "plt.subplot(321)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - all\")\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSI\")\n",
        "plt.subplot(323)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSI\")\n",
        "ds_hsv.reset_filter()\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSS\")\n",
        "plt.subplot(325)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSS\")\n",
        "\n",
        "\n",
        "ds_rgb = PatchDataset('./../dataset_ds')\n",
        "plt.subplot(322)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - all\")\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSI\")\n",
        "plt.subplot(324)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSI\")\n",
        "ds_rgb.reset_filter()\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSS\")\n",
        "plt.subplot(326)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "VkN0MKOcX3yg",
        "outputId": "8daf9858-a08d-4945-a421-69f28b5c7100"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 4000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8489c8a92b7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mds_hsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./../dataset_ds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb2hsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m321\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpatch_stat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_hsv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HSV - all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-e2c964b402c0>\u001b[0m in \u001b[0;36mpatch_stat\u001b[0;34m(ds)\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c0c209ca38dc>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \"\"\"\n\u001b[1;32m    222\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py\u001b[0m in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimage_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x864 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAADjCAYAAAD5XX9cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANTUlEQVR4nO3cYajd9X3H8ffHpFmZtXYst1CSWB2Ls8EOdBdxFFaHbsQ8SB50KwlIZxED3SxjlYKjwxb7qJN1UMhmMyauhWrTPigXmpIHrUUojeSKq5iI5S515qYFU2t9ImqzfffgnK7H603OP/f+z7m5+b1fcOH8/+d3z/ny4ybvnHNP/qkqJElq2WVrPYAkSWvNGEqSmmcMJUnNM4aSpOYZQ0lS84yhJKl5Y2OY5OEkLyV59hz3J8mXkiwkeSbJjf2PKUnS5HR5ZfgIsPM8998ObB9+7Qf+dfVjSZI0PWNjWFVPAL84z5I9wFdq4CjwniTv62tASZImrY/fGW4BTo0cLw7PSZK0Lmyc5pMl2c/grVQuv/zyP7ruuuum+fSSpEvYU0899fOqmlnJ9/YRw9PAtpHjrcNzb1NVB4GDALOzszU/P9/D00uSBEn+e6Xf28fbpHPAx4afKr0ZeLWqftbD40qSNBVjXxkmeRS4BdicZBH4LPAOgKp6CDgM7AIWgNeAj09qWEmSJmFsDKtq35j7C/ib3iaSJGnKvAKNJKl5xlCS1DxjKElqnjGUJDXPGEqSmmcMJUnNM4aSpOYZQ0lS84yhJKl5xlCS1DxjKElqnjGUJDXPGEqSmmcMJUnNM4aSpOYZQ0lS84yhJKl5xlCS1DxjKElqnjGUJDXPGEqSmmcMJUnNM4aSpOYZQ0lS84yhJKl5xlCS1DxjKElqnjGUJDXPGEqSmmcMJUnN6xTDJDuTPJ9kIcl9y9x/VZLHkzyd5Jkku/ofVZKkyRgbwyQbgAPA7cAOYF+SHUuW/QNwqKpuAPYC/9L3oJIkTUqXV4Y3AQtVdbKq3gQeA/YsWVPAu4e3rwR+2t+IkiRNVpcYbgFOjRwvDs+N+hxwR5JF4DDwyeUeKMn+JPNJ5s+cObOCcSVJ6l9fH6DZBzxSVVuBXcBXk7ztsavqYFXNVtXszMxMT08tSdLqdInhaWDbyPHW4blRdwGHAKrqh8A7gc19DChJ0qR1ieExYHuSa5JsYvABmbkla14EbgVI8gEGMfR9UEnSujA2hlV1FrgHOAI8x+BTo8eTPJBk93DZvcDdSX4EPArcWVU1qaElSerTxi6Lquowgw/GjJ67f+T2CeBD/Y4mSdJ0eAUaSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNa9TDJPsTPJ8koUk951jzUeTnEhyPMnX+h1TkqTJ2ThuQZINwAHgz4BF4FiSuao6MbJmO/D3wIeq6pUk753UwJIk9a3LK8ObgIWqOllVbwKPAXuWrLkbOFBVrwBU1Uv9jilJ0uR0ieEW4NTI8eLw3KhrgWuT/CDJ0SQ7+xpQkqRJG/s26QU8znbgFmAr8ESSD1bVL0cXJdkP7Ae46qqrenpqSZJWp8srw9PAtpHjrcNzoxaBuar6VVX9BPgxgzi+RVUdrKrZqpqdmZlZ6cySJPWqSwyPAduTXJNkE7AXmFuy5lsMXhWSZDODt01P9jinJEkTMzaGVXUWuAc4AjwHHKqq40keSLJ7uOwI8HKSE8DjwKer6uVJDS1JUp9SVWvyxLOzszU/P78mzy1JuvQkeaqqZlfyvV6BRpLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc3rFMMkO5M8n2QhyX3nWfeRJJVktr8RJUmarLExTLIBOADcDuwA9iXZscy6K4C/BZ7se0hJkiapyyvDm4CFqjpZVW8CjwF7lln3eeALwOs9zidJ0sR1ieEW4NTI8eLw3P9LciOwraq+3eNskiRNxao/QJPkMuCLwL0d1u5PMp9k/syZM6t9akmSetElhqeBbSPHW4fnfu0K4Hrg+0leAG4G5pb7EE1VHayq2aqanZmZWfnUkiT1qEsMjwHbk1yTZBOwF5j79Z1V9WpVba6qq6vqauAosLuq5icysSRJPRsbw6o6C9wDHAGeAw5V1fEkDyTZPekBJUmatI1dFlXVYeDwknP3n2PtLasfS5Kk6fEKNJKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNa9TDJPsTPJ8koUk9y1z/6eSnEjyTJLvJnl//6NKkjQZY2OYZANwALgd2AHsS7JjybKngdmq+kPgm8A/9j2oJEmT0uWV4U3AQlWdrKo3gceAPaMLqurxqnpteHgU2NrvmJIkTU6XGG4BTo0cLw7PnctdwHeWuyPJ/iTzSebPnDnTfUpJkiao1w/QJLkDmAUeXO7+qjpYVbNVNTszM9PnU0uStGIbO6w5DWwbOd46PPcWSW4DPgN8uKre6Gc8SZImr8srw2PA9iTXJNkE7AXmRhckuQH4MrC7ql7qf0xJkiZnbAyr6ixwD3AEeA44VFXHkzyQZPdw2YPAu4BvJPnPJHPneDhJki46Xd4mpaoOA4eXnLt/5PZtPc8lSdLUeAUaSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNc8YSpKaZwwlSc0zhpKk5hlDSVLzjKEkqXnGUJLUPGMoSWqeMZQkNa9TDJPsTPJ8koUk9y1z/28l+frw/ieTXN33oJIkTcrYGCbZABwAbgd2APuS7Fiy7C7glar6feCfgS/0PagkSZPS5ZXhTcBCVZ2sqjeBx4A9S9bsAf5jePubwK1J0t+YkiRNTpcYbgFOjRwvDs8tu6aqzgKvAr/bx4CSJE3axmk+WZL9wP7h4RtJnp3m81+CNgM/X+sh1jn3sB/u4+q5h6v3Byv9xi4xPA1sGzneOjy33JrFJBuBK4GXlz5QVR0EDgIkma+q2ZUMrQH3cPXcw364j6vnHq5ekvmVfm+Xt0mPAduTXJNkE7AXmFuyZg74q+HtvwC+V1W10qEkSZqmsa8Mq+psknuAI8AG4OGqOp7kAWC+quaAfwe+mmQB+AWDYEqStC50+p1hVR0GDi85d//I7deBv7zA5z54gev1du7h6rmH/XAfV889XL0V72F8N1OS1DovxyZJat7EY+il3Favwx5+KsmJJM8k+W6S96/FnBezcXs4su4jSSqJn+pbosseJvno8GfxeJKvTXvGi12HP8tXJXk8ydPDP8+71mLOi1mSh5O8dK7/mpeBLw33+JkkN3Z64Kqa2BeDD9z8F/B7wCbgR8COJWv+GnhoeHsv8PVJzrTevjru4Z8Cvz28/Qn38ML3cLjuCuAJ4Cgwu9ZzX0xfHX8OtwNPA78zPH7vWs99MX113MODwCeGt3cAL6z13BfbF/AnwI3As+e4fxfwHSDAzcCTXR530q8MvZTb6o3dw6p6vKpeGx4eZfB/QfUbXX4OAT7P4Lq6r09zuHWiyx7eDRyoqlcAquqlKc94seuyhwW8e3j7SuCnU5xvXaiqJxj8r4Vz2QN8pQaOAu9J8r5xjzvpGHopt9Xrsoej7mLwryL9xtg9HL6Vsq2qvj3NwdaRLj+H1wLXJvlBkqNJdk5tuvWhyx5+DrgjySKDT/B/cjqjXVIu9O9MYMqXY9NkJbkDmAU+vNazrCdJLgO+CNy5xqOsdxsZvFV6C4N3J55I8sGq+uWaTrW+7AMeqap/SvLHDP7/9vVV9b9rPdilbtKvDC/kUm6c71JuDeuyhyS5DfgMsLuq3pjSbOvFuD28Arge+H6SFxj8nmHOD9G8RZefw0Vgrqp+VVU/AX7MII4a6LKHdwGHAKrqh8A7GVyzVN11+jtzqUnH0Eu5rd7YPUxyA/BlBiH09zRvd949rKpXq2pzVV1dVVcz+L3r7qpa8XUOL0Fd/ix/i8GrQpJsZvC26clpDnmR67KHLwK3AiT5AIMYnpnqlOvfHPCx4adKbwZeraqfjfumib5NWl7KbdU67uGDwLuAbww/e/RiVe1es6EvMh33UOfRcQ+PAH+e5ATwP8Cnq8p3eYY67uG9wL8l+TsGH6a50xcHb5XkUQb/6No8/N3qZ4F3AFTVQwx+17oLWABeAz7e6XHdZ0lS67wCjSSpecZQktQ8YyhJap4xlCQ1zxhKkppnDCVJzTOGkqTmGUNJUvP+Dw+RPDu06+XZAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature dimensaion reduction"
      ],
      "metadata": {
        "id": "jKeus-71JvTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_vec(mat, vec):\n",
        "  print(\"shape mat\", mat.shape)\n",
        "  print(\"shape vec\", vec.shape)\n",
        "  return mat\n",
        "\n",
        "\n",
        "def batch_PCA(ftrs, k=2, mean=False, eig_val=False):\n",
        "  #print(\"Batch PCA input size: {}\".format(ftrs.shape))\n",
        "  ftrs = ftrs.detach().cpu().numpy()\n",
        "  #ftr_var = (ftrs.T @ ftrs).cpu().numpy()\n",
        "  PCA_comp = PCA(k, svd_solver='full').fit(ftrs)\n",
        "  eig_vec = torch.Tensor(PCA_comp.components_)\n",
        "  if mean:\n",
        "    mean = torch.mean(torch.from_numpy(ftrs), dim=0)\n",
        "    eig_vec = torch.vstack((eig_vec, mean))\n",
        "  if eig_val:\n",
        "    eig_val = PCA_comp.singular_values_\n",
        "    eig_vec = add_vec(eig_vec, eig_val)\n",
        "  #print(\"Eigen Vector size :{} , with max: {}\".format(eig_vec.shape, eig_vec.max()))\n",
        "  return eig_vec"
      ],
      "metadata": {
        "id": "WQ5gJRu8-oF0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension reduction "
      ],
      "metadata": {
        "id": "q7AfgojaKAQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def incpt3_out2():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.fc = model.fc[0]\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def incpt3_out100():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def patient_data(dir, nd, model_fnc, dr_fnc, trf=None):\n",
        "  model = model_fnc()\n",
        "  ds = PatchDataset(dir, transform=trf)\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ds.get_sets():\n",
        "    s = mkdir(nd, t)\n",
        "    for m in ds.get_labels():\n",
        "     d = mkdir(s, m)\n",
        "     ds.set_filter(ds=t, lb=m)\n",
        "     for p, sz in ds.get_patients(num=True):\n",
        "       print(\"Patient {} - of group {}-{} has {} patches\".format(p,t,m,sz))\n",
        "       ds.set_filter(ds=t, lb=m, pt=p)\n",
        "       fname = os.path.join(d, p + '.npy')\n",
        "       if (len(ds) >= 2) and not os.path.isfile(fname):\n",
        "        dr = dr_fnc(ds, model).cpu().numpy()\n",
        "        with open(fname, 'wb') as f:\n",
        "           np.save(f, dr)\n",
        "       ds.reset_filter()\n",
        "     ds.reset_filter()\n",
        "\n",
        "\n",
        "def patientPCA(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return batch_PCA(ftrs, k=2, mean=False, eig_val=False)\n",
        "\n",
        "\n",
        "def patientRes(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return ftrs\n",
        "\n",
        "\n",
        "def read_mtx(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    return np.load(f)"
      ],
      "metadata": {
        "id": "uc30a7QqwPc9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Part"
      ],
      "metadata": {
        "id": "vBUT9hMun8Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, head, rest):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encode = head\n",
        "    self.decode = rest\n",
        "  \n",
        "  def forward(self, imgs):\n",
        "    x = self.encode(imgs)\n",
        "    return self.decode(x)\n",
        "\n",
        "  def feature_ext(self, imgs):\n",
        "    return self.encode(imgs)"
      ],
      "metadata": {
        "id": "8ReMGN_XAxsF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class vgg16_features(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg16_features, self).__init__()\n",
        "    self.m = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "  def forward(self, imgs):\n",
        "    ftrs = self.m.features(imgs)\n",
        "    return torch.max(ftrs, dim=1).values\n",
        "    "
      ],
      "metadata": {
        "id": "La5H9qOey8Vs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPmodel(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(MLPmodel, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, input//8),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(input//8, 2), \n",
        "        nn.Softmax())\n",
        "\n",
        "    \n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    return self.mlp(ev_)\n",
        "\n",
        "\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, 2, bias=True),\n",
        "        nn.Softmax(dim=1))\n",
        "\n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    ev_ = self.mlp(ev_)\n",
        "    return ev_\n",
        "\n",
        "\n",
        "\n",
        "def calculate_accuracy(model, dataloader, criterion, device):\n",
        "    \"\"\"Used on a set model to gain loss and accuracy, does not train\"\"\"\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader: # Batch wise check\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images) # Run the images through the net\n",
        "            _, predicted = torch.max(outputs.data, 1) # Top  result\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += criterion(outputs, labels).data.item() / len(dataloader)\n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, total_loss\n",
        "\n",
        "\n",
        "def train(hp):\n",
        "  \n",
        "  model = hp[\"model\"].to(\"cpu\")\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= hp['lr'])\n",
        "  ds_train, ds_test = hp['dataset']\n",
        "  ld_train = torch.utils.data.DataLoader(ds_train, batch_size=hp['bts'], shuffle=True)\n",
        "  ld_test = torch.utils.data.DataLoader(ds_test, batch_size=hp['bts'], shuffle=True)\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "    # training loop\n",
        "  train_error = []\n",
        "  test_error = []\n",
        "  for epoch in range(1, hp['epochs'] + 1):\n",
        "      model.train()  # put in training mode\n",
        "      running_loss = 0.0\n",
        "      epoch_time = time.time()\n",
        "      for i, data in enumerate(ld_train, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          # send them to device\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          #ref = torch.Tensor(1, 1)\n",
        "          #ref[0] = labels\n",
        "          #labels = ref\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)  # forward pass\n",
        "          loss = criterion(outputs, labels)  # calculate the loss\n",
        "          # always the same 3 steps\n",
        "          optimizer.zero_grad()  # zero the parameter gradients\n",
        "          loss.backward()  # backpropagation\n",
        "          optimizer.step()  # update parameters\n",
        "\n",
        "          # Add statistics\n",
        "          train_error.append(loss.data.item())\n",
        "          running_loss += loss.data.item()\n",
        "\n",
        "      # Advance in learning rate schedule\n",
        "      # Normalizing the loss by the total number of train batches\n",
        "      running_loss /= len(ld_train)\n",
        "\n",
        "      # Calculate training/test set accuracy of the existing model\n",
        "      train_accuracy, _= calculate_accuracy(model, ld_train, criterion, device)\n",
        "      test_accuracy, test_loss = calculate_accuracy(model, ld_test, criterion, device)\n",
        "      test_error.append(test_loss)\n",
        "      log = \"Epoch: {:2d} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "      #train_error.append(running_loss)\n",
        "      epoch_time = time.time() - epoch_time\n",
        "      log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "      print(log)\n",
        "    \n",
        "  print('==> Finished Training ...')\n",
        "\n",
        "  # Graph test and train loss across iterations\n",
        "  batches = len(ld_train)\n",
        "  itr = list(range(hp['epochs'] * batches))\n",
        "  plt.plot(itr, train_error, 'r--', label='Train batch loss', linewidth=0.5)\n",
        "  itr = list(range(batches,(hp['epochs'] * len(ld_train)+batches), batches))\n",
        "  plt.plot(itr, test_error, color='g', label='Test Epoch loss', linewidth= 3)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylim(0, max(train_error))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # save model\n",
        "  print('==> Saving model ...')\n",
        "  state = {\n",
        "      'net': model.state_dict(),\n",
        "      'epoch': epoch,\n",
        "  }\n",
        "  if not os.path.isdir('checkpoints'):\n",
        "      os.mkdir('checkpoints')\n",
        "  torch.save(state, './checkpoints/' + hp['name'] +'.pth')\n",
        "   "
      ],
      "metadata": {
        "id": "Wgw2EMQ3DBm_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Networks"
      ],
      "metadata": {
        "id": "LdmB6u0loBwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process(dir_train, dir_test , model, name):\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  dataset = []\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_train, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "  dataset.append(torchvision.datasets.DatasetFolder(root=dir_test, loader=read_mtx, extensions='.npy', transform=transform))\n",
        "\n",
        "  hyper_parameters = {\n",
        "      'model': model,\n",
        "      'lr': 1e-3,\n",
        "      'bts': 1,\n",
        "      'dataset': dataset,\n",
        "      'epochs': 20,\n",
        "      'name': name\n",
        "  }\n",
        "  train(hyper_parameters)\n",
        "\n",
        "\n",
        "def exp_pca(ftrs):\n",
        "  return batch_PCA(torch.exp(ftrs))\n",
        "\n",
        "\n",
        "def log_pca(ftrs):\n",
        "  return batch_PCA(torch.log(ftrs))"
      ],
      "metadata": {
        "id": "DGNsd8h-TBtE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_res(dir):\n",
        "  ds = torchvision.datasets.DatasetFolder(root=dir, loader=read_mtx, extensions='.npy')\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  for i, data in enumerate(dl):\n",
        "    if i < 1:\n",
        "      mat, lb = data\n",
        "      lb= int(lb.cpu())\n",
        "      mat = mat.numpy()[0]\n",
        "      ln = len(mat[:,0])\n",
        "      cl = 0 if lb == 0 else 90\n",
        "      cl = [cl + int(10*l/ln) for l in range(ln)]\n",
        "      print(cl)\n",
        "      plt.scatter(mat[:,0], mat[:,1], c=cl)\n",
        "  plt.show()\n",
        "  \n",
        "show_res('../dataset_2d_matrix_mm')"
      ],
      "metadata": {
        "id": "0XOtJPuHyN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_2d_res', incpt3_out2, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "U6uLbdNpMFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_100d_res', incpt3_out100, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "5bjNG4Y1BTmA",
        "outputId": "c7754699-eb05-44e5-e668-8e8c788b6c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "found 192399\n",
            "Patient G4-6302 - of group test-MSI has 247 patches\n",
            "Patient CK-6747 - of group test-MSI has 1765 patches\n",
            "Patient D5-6530 - of group test-MSI has 3105 patches\n",
            "Patient AA-3966 - of group test-MSI has 854 patches\n",
            "Patient A6-2686 - of group test-MSI has 681 patches\n",
            "Patient CM-4743 - of group test-MSI has 4064 patches\n",
            "Patient AD-6964 - of group test-MSI has 831 patches\n",
            "Patient AD-A5EJ - of group test-MSI has 507 patches\n",
            "Patient AZ-4615 - of group test-MSI has 2051 patches\n",
            "Patient CK-6746 - of group test-MSI has 2438 patches\n",
            "Patient AZ-6601 - of group test-MSI has 1569 patches\n",
            "Patient AA-3950 - of group test-MSI has 508 patches\n",
            "Patient AZ-6598 - of group test-MSI has 1788 patches\n",
            "Patient AA-3877 - of group test-MSI has 452 patches\n",
            "Patient D5-6928 - of group test-MSI has 80 patches\n",
            "Patient AY-6197 - of group test-MSI has 2090 patches\n",
            "Patient G4-6309 - of group test-MSI has 240 patches\n",
            "Patient CK-4951 - of group test-MSI has 720 patches\n",
            "Patient DC-6154 - of group test-MSI has 1250 patches\n",
            "Patient AG-A02N - of group test-MSI has 490 patches\n",
            "Patient D5-6540 - of group test-MSI has 345 patches\n",
            "Patient G4-6588 - of group test-MSI has 523 patches\n",
            "Patient G4-6586 - of group test-MSI has 730 patches\n",
            "Patient AD-5900 - of group test-MSI has 361 patches\n",
            "Patient WS-AB45 - of group test-MSI has 343 patches\n",
            "Patient AA-3833 - of group test-MSI has 303 patches\n",
            "Patient AG-3898 - of group test-MSS has 648 patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_2d_matrix_mm/train', '../dataset_2d_matrix_mm/test', LinearReg(4, exp_pca), \"LR_test\")"
      ],
      "metadata": {
        "id": "gWPSlWJgAljl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grave yard"
      ],
      "metadata": {
        "id": "xg44H4J-Um71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class patches_DataSet(torchvision.datasets.DatasetFolder):\n",
        "  def __init__(self, root='./data'):\n",
        "    super(torchvision.datasets.DatasetFolder, self).init__(root=root, loader=torchvision.io.read_image, extensions='.png')\n",
        "\n",
        "  def find_classes(dir):\n",
        "    return None"
      ],
      "metadata": {
        "id": "5aqZoNEuQTRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(ds, dir):\n",
        "  cls = ds.find_classes(dir)[0]\n",
        "  ret = []\n",
        "  for c in cls:\n",
        "    case = c[-1]\n",
        "    ret.append(case)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "zLBpwMzK0AC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Project_jupyter.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}