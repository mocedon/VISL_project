{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Structure:\n",
        "\n",
        "- Get images\n",
        "  - Per patient\n",
        "  - Train\n",
        "  - Test\n",
        "-Run all images through feature detection\n",
        "-Take each patient, run PCA with k =?\n",
        "-Run MLP classifier / linear discriminator"
      ],
      "metadata": {
        "id": "hGa51nBR-3Xf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and drive"
      ],
      "metadata": {
        "id": "c-Gxaaa6oZeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import kornia\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import cv2\n",
        "\n",
        "import re\n",
        "import json \n",
        "import glob\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "\n",
        "# %matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "hjYfD2Z1BMtV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to drive to save work\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/Proj_B\n",
        "#!git clone https://github.com/mocedon/VISL_project\n",
        "#!git pull\n",
        "%cd VISL_project"
      ],
      "metadata": {
        "id": "xWZR02PBun-1",
        "outputId": "7a13ec51-151a-49ae-fdb6-0d4947cb84e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Proj_B\n",
            "/content/gdrive/MyDrive/Proj_B/VISL_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Goofing around"
      ],
      "metadata": {
        "id": "SEAEhMJi8Ako"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max([1,3,2,5,3,6,3,4,2,5,3,4])"
      ],
      "metadata": {
        "id": "xtt7KdNSRhhu",
        "outputId": "956fec23-1059-4e06-9833-999179666e0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = [1,2,3,4,5,6,7]\n",
        "l2 = l1\n",
        "l2 = [i for i in l2 if i%2 == 0]\n",
        "print(l1)\n",
        "print(l2)"
      ],
      "metadata": {
        "id": "gCvaumsKVY_j",
        "outputId": "fc7d76e3-d2b5-4e7d-f3b9-1c01fd44b01c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 7]\n",
            "[2, 4, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = [{\"t\":\"ts\" , \"m\":\"msi\", \"im\":0},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":1},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":2},\n",
        "     {\"t\":\"ts\" , \"m\":\"msi\", \"im\":3},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":4},\n",
        "     {\"t\":\"ts\" , \"m\":\"mss\", \"im\":5},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":6},\n",
        "     {\"t\":\"tr\" , \"m\":\"msi\", \"im\":7},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":8},\n",
        "     {\"t\":\"tr\" , \"m\":\"mss\", \"im\":9}]\n",
        "p = pd.DataFrame(d)\n",
        "p[(p[\"t\"]== 'ts')&(p[\"m\"]=='msi')]"
      ],
      "metadata": {
        "id": "R_2YFi638Dqs",
        "outputId": "1dad0306-2357-4f8e-e973-c000229cc6ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fadc20fa-63ce-41f5-af99-46cff94549e6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>t</th>\n",
              "      <th>m</th>\n",
              "      <th>im</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ts</td>\n",
              "      <td>msi</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fadc20fa-63ce-41f5-af99-46cff94549e6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fadc20fa-63ce-41f5-af99-46cff94549e6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    t    m  im\n",
              "0  ts  msi   0\n",
              "3  ts  msi   3"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.rand(3,16,16)\n",
        "print(\"torch shape\", t.shape)\n",
        "print(\"np shape\", t.numpy().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CcHSoX8aFkj",
        "outputId": "3638d54f-a07f-4c49-e955-66718a8ec62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch shape torch.Size([3, 16, 16])\n",
            "np shape (3, 16, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = None\n",
        "a = torch.Tensor([1,2,3])\n",
        "b = torch.Tensor([4,5,6])\n",
        "t = torch.vstack((t,a)) if t else a\n",
        "t = torch.vstack((t,b,a,b)) \n",
        "print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3875yI9hfqz",
        "outputId": "15f25c1a-e0e3-448a-ca9b-94220829c953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.],\n",
            "        [1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fake data"
      ],
      "metadata": {
        "id": "Mam2ZqAVoe-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fake_data(dir = './data', ptns=12, pchs=10):\n",
        "  imgs = torch.rand((ptns, pchs,3, 224,224))\n",
        "  os.mkdir(dir)\n",
        "  classes = []\n",
        "  for p in range(ptns):\n",
        "    case = 'A' if torch.randn(1) > 0 else 'B'\n",
        "    classes.append(case)\n",
        "    p_name = \"ptn-{}-{}\".format(str(p).zfill(3), case)\n",
        "    pth = os.path.join(dir, p_name)\n",
        "    os.mkdir(pth)\n",
        "    for i in range(pchs):\n",
        "      im = imgs[int(p),i]\n",
        "      if case is 'B':\n",
        "        im = im / 2\n",
        "      fname = os.path.join(dir ,p_name, str(i).zfill(3) + '.png')\n",
        "      torchvision.utils.save_image(im, fname)\n",
        "  return classes"
      ],
      "metadata": {
        "id": "vT4M9f7I-e2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr data*\n",
        "!rm -fr aug_data*\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT2ybGgdSEBI",
        "outputId": "49454393-1f4f-4f8e-b4bb-42c421600e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './data'\n",
        "classes = fake_data(dir)\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnmUlwtB1JI",
        "outputId": "4d0de4c2-2e6c-4760-c17a-539668d6c57f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_PCA_(imgs ,k=16):\n",
        "  print(\"batch input size: {}\".format(imgs.shape))\n",
        "  ret = imgs.mean(dim=0)\n",
        "  print(\"batch new size: {}\".format(ret.shape))\n",
        "  return ret"
      ],
      "metadata": {
        "id": "wjgM5J7Ln2Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pwd"
      ],
      "metadata": {
        "id": "tnWuDvFQAK9a",
        "outputId": "ed194e8a-123e-4243-b502-461f21f6d80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/gdrive/MyDrive/Proj_B/VISL_project'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Util Functions"
      ],
      "metadata": {
        "id": "h9IVl3Rdrk5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_img(imgs):\n",
        "  s = imgs.shape[-1] // 8\n",
        "  t = transforms.RandomCrop(s)\n",
        "  return t(imgs)"
      ],
      "metadata": {
        "id": "tULoXSwVAQpN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "  dev = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "  device = torch.device(dev)\n",
        "  return device"
      ],
      "metadata": {
        "id": "DmE9FA8Grg3R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_patches(fn ,id):\n",
        "  dir, bn = os.path.split(fn)\n",
        "  patient = '-'.join(bn.split('-')[3:5])\n",
        "  dir, label = os.path.split(dir)\n",
        "  dir, ds = os.path.split(dir)\n",
        "  return {'id': id,\n",
        "          'file_name': fn,\n",
        "          'set': ds,\n",
        "          'label': label,\n",
        "          'patient': patient}"
      ],
      "metadata": {
        "id": "PKUYbyo3Df3k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir(dir_, dir):\n",
        "  \"\"\"Creeates directory if necessary and returns dir path\"\"\"\n",
        "  d = os.path.join(dir_,dir)\n",
        "  if not os.path.isdir(d):\n",
        "    os.mkdir(d)\n",
        "  return d"
      ],
      "metadata": {
        "id": "nKlyd25jVaan"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ds_data(dir, nd, n):\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ['test', 'train']:\n",
        "    d = mkdir(nd, t)\n",
        "    for m in ['MSI', 'MSS']:\n",
        "      mkdir(d,m)\n",
        "      path = os.path.join(dir, t, m)\n",
        "      print(path)\n",
        "      for im in glob.glob(path + '/*.png')[:n]:\n",
        "        #print(im)\n",
        "        name = os.path.basename(dir)\n",
        "        repl = os.path.basename(nd)\n",
        "        np = im.replace(name, repl)\n",
        "        #print(np)\n",
        "        if not os.path.isfile(np):\n",
        "          shutil.copy(im, np)\n",
        "\n",
        "#ds_data('./../dataset', './../small_dataset', 10000)\n",
        "#ds_data('./../dataset', './../dataset_ds', 1000)"
      ],
      "metadata": {
        "id": "HAZO6Al6_Djz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation"
      ],
      "metadata": {
        "id": "9LnLAM_sq0rC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DS Class"
      ],
      "metadata": {
        "id": "2XHqOXIVJhf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchDataset(Dataset):\n",
        "  def __init__(self, dir, parse_fn=parse_patches, transform=None):\n",
        "    self.patch_list = []\n",
        "    for i, fn in enumerate(glob.glob(dir + \"/*/*/*.png\")):\n",
        "      self.patch_list.append(parse_fn(fn, i))\n",
        "    self.active_list = self.patch_list.copy()\n",
        "    self.patient_dict = None\n",
        "    self.transform = transform\n",
        "    print(\"found \"+str(len(self.patch_list)))\n",
        "\n",
        "  def set_filter(self, ds=None, lb=None, pt=None):\n",
        "    if (ds):\n",
        "      self.active_list = [p for p in self.active_list if p['set'] == ds]\n",
        "    if (lb):\n",
        "      self.active_list = [p for p in self.active_list if p['label'] == lb]\n",
        "    if (pt):\n",
        "      self.active_list = [p for p in self.active_list if p['patient'] == pt]\n",
        "  \n",
        "  def reset_filter(self):\n",
        "    self.active_list = self.patch_list.copy()\n",
        "  \n",
        "  def get_sct(self, sect, num=False):\n",
        "    l_ = [p[sect] for p in self.active_list]\n",
        "    l_dct = {}\n",
        "    for l in l_:\n",
        "      l_dct[l] = l_dct.get(l, 0) + 1\n",
        "    return l_dct.items() if num else l_dct.keys()\n",
        "\n",
        "  def get_patients(self, num=False):\n",
        "    return self.get_sct('patient', num)\n",
        "\n",
        "  def get_sets(self, num=False):\n",
        "    return self.get_sct('set', num)\n",
        "  \n",
        "  def get_labels(self, num=False):\n",
        "    return self.get_sct('label', num)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.active_list)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    fn = self.active_list[index][\"file_name\"]\n",
        "    lb = self.active_list[index]['label']\n",
        "    img = torchvision.io.read_image(fn)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return (img, lb)"
      ],
      "metadata": {
        "id": "xcISDAO3AGnw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension reduction "
      ],
      "metadata": {
        "id": "q7AfgojaKAQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_vec(mat, vec):\n",
        "  print(\"shape mat\", mat.shape)\n",
        "  print(\"shape vec\", vec.shape)\n",
        "  return mat\n",
        "\n",
        "\n",
        "class batch_PCA():\n",
        "  def __init__(self, k=2, mean=False, eig_val=False, fnc=None):\n",
        "    self.k = k\n",
        "    self.mean = mean\n",
        "    self.eig_val = eig_val\n",
        "    self.fnc = fnc\n",
        "\n",
        "  def __call__(self, ftrs):\n",
        "    if self.fnc is not None:\n",
        "      ftrs = self.fnc(ftrs)\n",
        "    #print(\"Batch PCA input size: {}\".format(ftrs.shape))\n",
        "    p, f = ftrs.shape\n",
        "    if p < f:\n",
        "      avg = torch.mean(ftrs, dim=0)\n",
        "      for i in range(int(p) - int(f)):\n",
        "        ftrs = torch.vstack((ftrs, avg))\n",
        "    ftrs = ftrs.detach().cpu().numpy()\n",
        "    #ftr_var = (ftrs.T @ ftrs).cpu().numpy()\n",
        "    PCA_comp = PCA(self.k, svd_solver='full').fit(ftrs)\n",
        "    eig_vec = torch.Tensor(PCA_comp.components_)\n",
        "    if self.mean:\n",
        "      mean = torch.mean(torch.from_numpy(ftrs), dim=0)\n",
        "      eig_vec = torch.vstack((eig_vec, mean))\n",
        "    if self.eig_val:\n",
        "      eig_val = torch.from_numpy(PCA_comp.singular_values_)\n",
        "      eig_vec = torch.vstack((eig_vec, eig_val))\n",
        "    #print(\"Eigen Vector size :{} , with max: {}\".format(eig_vec.shape, eig_vec.max()))\n",
        "    return eig_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "WQ5gJRu8-oF0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def incpt3_out2():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def incpt3_out100():\n",
        "  model = models.inception_v3(pretrained=True)\n",
        "  model.fc = nn.Sequential(nn.Linear(2048,100), nn.Linear(100,2))\n",
        "  weights = torch.load('./../results/2022_03_13__22_59_56/checkpoints/epoch_17.pth', map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(weights['net'])\n",
        "  model.fc = model.fc[0]\n",
        "  model.to(get_device())\n",
        "  model.eval()\n",
        "  return model\n",
        "\n",
        "\n",
        "def patient_data(dir, nd, model_fnc, dr_fnc, trf=None):\n",
        "  model = model_fnc()\n",
        "  ds = PatchDataset(dir, transform=trf)\n",
        "  nd = mkdir('./', nd)\n",
        "  for t in ds.get_sets():\n",
        "    s = mkdir(nd, t)\n",
        "    for m in ds.get_labels():\n",
        "     d = mkdir(s, m)\n",
        "     ds.set_filter(ds=t, lb=m)\n",
        "     for p, sz in ds.get_patients(num=True):\n",
        "       print(\"Patient {} - of group {}-{} has {} patches\".format(p,t,m,sz))\n",
        "       ds.set_filter(ds=t, lb=m, pt=p)\n",
        "       fname = os.path.join(d, p + '.npy')\n",
        "       if (len(ds) >= 2) and not os.path.isfile(fname):\n",
        "        dr = dr_fnc(ds, model).cpu().numpy()\n",
        "        with open(fname, 'wb') as f:\n",
        "           np.save(f, dr)\n",
        "       ds.reset_filter()\n",
        "     ds.reset_filter()\n",
        "\n",
        "\n",
        "def patientPCA(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return batch_PCA(ftrs, k=2, mean=False, eig_val=False)\n",
        "\n",
        "\n",
        "def patientRes(ds, model):\n",
        "  bt_sz = 64\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=bt_sz, shuffle=False, num_workers=4)\n",
        "  ftrs = None\n",
        "  with torch.no_grad():\n",
        "    for data in dl:\n",
        "      imgs = data[0].to(get_device())\n",
        "      ftr = model(imgs)\n",
        "      ftrs = torch.vstack((ftrs, ftr)) if ftrs is not None else ftr\n",
        "  return ftrs\n",
        "\n",
        "\n",
        "def read_mtx(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    return np.load(f)"
      ],
      "metadata": {
        "id": "uc30a7QqwPc9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Part"
      ],
      "metadata": {
        "id": "vBUT9hMun8Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPmodel(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(MLPmodel, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, input),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(input, 2), \n",
        "        nn.Softmax(dim=1))\n",
        "\n",
        "    \n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    return self.mlp(ev_)\n",
        "\n",
        "\n",
        "class LinearReg(nn.Module):\n",
        "  def __init__(self, input, fnc):\n",
        "    super(LinearReg, self).__init__()\n",
        "    self.fnc = fnc\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input, 2, bias=True),\n",
        "        nn.Softmax(dim=1))\n",
        "\n",
        "  def forward(self, ev):\n",
        "    ev = self.fnc(ev[0, 0])\n",
        "    ev = torch.flatten(ev)\n",
        "    ev_ = torch.Tensor(1, len(ev))\n",
        "    ev_[0] = ev_\n",
        "    ev_ = self.mlp(ev_)\n",
        "    return ev_"
      ],
      "metadata": {
        "id": "KRwgoPbuuZuI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class equalDataset(Dataset):\n",
        "  def __init__(self, dir, transform=None):\n",
        "    self.classes = {}\n",
        "    cl_lb = {}\n",
        "    glob_list = glob.glob(dir + \"/*/*.npy\")\n",
        "    for i, fn in enumerate(glob_list):\n",
        "      dir, bn = os.path.split(fn)\n",
        "      dir, label = os.path.split(dir)\n",
        "      if label not in cl_lb.keys():\n",
        "        cl_lb[label] = len(self.classes.keys())\n",
        "        self.classes[cl_lb[label]] = []\n",
        "      self.classes[cl_lb[label]].append({'pt': fn, 'id': i})\n",
        "    self.max_cl = max([len(p) for p in self.classes.values()])\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    max_cl = self.max_cl\n",
        "    return max_cl * len(self.classes.keys())\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    lb = index // self.max_cl\n",
        "    i = index % len(self.classes[lb])\n",
        "    fn = self.classes[lb][i][\"pt\"]\n",
        "    img = read_mtx(fn)\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return (img, lb)"
      ],
      "metadata": {
        "id": "QuLlWWMHHYrh"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Q_pswhil1ogM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, criterion, device):\n",
        "    \"\"\"Used on a set model to gain loss and accuracy, does not train\"\"\"\n",
        "    model.eval() # put in evaluation mode\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader: # Batch wise check\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images) # Run the images through the net\n",
        "            _, predicted = torch.max(outputs.data, 1) # Top  result\n",
        "            total_images += labels.size(0)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_loss += criterion(outputs, labels).data.item() / len(dataloader)\n",
        "\n",
        "    model_accuracy = total_correct / total_images * 100\n",
        "    return model_accuracy, total_loss\n",
        "\n",
        "\n",
        "def train(hp):\n",
        "  \n",
        "  model = hp[\"model\"].to(\"cpu\")\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr= hp['lr'])\n",
        "  ds_train, ds_test = hp['dataset']\n",
        "  ld_train = torch.utils.data.DataLoader(ds_train, batch_size=hp['bts'], shuffle=True)\n",
        "  ld_test = torch.utils.data.DataLoader(ds_test, batch_size=hp['bts'], shuffle=True)\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "    # training loop\n",
        "  train_error = []\n",
        "  test_error = []\n",
        "  for epoch in range(1, hp['epochs'] + 1):\n",
        "      model.train()  # put in training mode\n",
        "      running_loss = 0.0\n",
        "      epoch_time = time.time()\n",
        "      for i, data in enumerate(ld_train, 0):\n",
        "          # get the inputs\n",
        "          inputs, labels = data\n",
        "          # send them to device\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.to(device)\n",
        "          #ref = torch.Tensor(1, 1)\n",
        "          #ref[0] = labels\n",
        "          #labels = ref\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          outputs = model(inputs)  # forward pass\n",
        "          loss = criterion(outputs, labels)  # calculate the loss\n",
        "          # always the same 3 steps\n",
        "          optimizer.zero_grad()  # zero the parameter gradients\n",
        "          loss.backward()  # backpropagation\n",
        "          optimizer.step()  # update parameters\n",
        "\n",
        "          # Add statistics\n",
        "          train_error.append(loss.data.item())\n",
        "          running_loss += loss.data.item()\n",
        "\n",
        "      # Advance in learning rate schedule\n",
        "      # Normalizing the loss by the total number of train batches\n",
        "      running_loss /= len(ld_train)\n",
        "\n",
        "      # Calculate training/test set accuracy of the existing model\n",
        "      train_accuracy, _= calculate_accuracy(model, ld_train, criterion, device)\n",
        "      test_accuracy, test_loss = calculate_accuracy(model, ld_test, criterion, device)\n",
        "      test_error.append(test_loss)\n",
        "      log = \"Epoch: {:2d} | Loss: {:.4f} | Training accuracy: {:.3f}% | Test accuracy: {:.3f}% | \".format(epoch, running_loss, train_accuracy, test_accuracy)\n",
        "      #train_error.append(running_loss)\n",
        "      epoch_time = time.time() - epoch_time\n",
        "      log += \"Epoch Time: {:.2f} secs\".format(epoch_time)\n",
        "      print(log)\n",
        "    \n",
        "  print('==> Finished Training ...')\n",
        "\n",
        "  # Graph test and train loss across iterations\n",
        "  batches = len(ld_train)\n",
        "  itr = list(range(hp['epochs'] * batches))\n",
        "  plt.plot(itr, train_error, 'r--', label='Train batch loss', linewidth=0.5)\n",
        "  itr = list(range(batches,(hp['epochs'] * len(ld_train)+batches), batches))\n",
        "  plt.plot(itr, test_error, color='g', label='Test Epoch loss', linewidth= 3)\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Batch')\n",
        "  plt.ylim(0, max(train_error))\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # save model\n",
        "  print('==> Saving model ...')\n",
        "  state = {\n",
        "      'net': model.state_dict(),\n",
        "      'epoch': epoch,\n",
        "  }\n",
        "  if not os.path.isdir('checkpoints'):\n",
        "      os.mkdir('checkpoints')\n",
        "  torch.save(state, './checkpoints/' + hp['name'] +'.pth')\n",
        "   "
      ],
      "metadata": {
        "id": "Wgw2EMQ3DBm_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(dir_train, dir_test , model, name):\n",
        "  transform = transforms.Compose([transforms.ToTensor()])\n",
        "  dataset = []\n",
        "  dataset.append(equalDataset(dir=dir_train, transform=transform))\n",
        "  dataset.append(equalDataset(dir=dir_test, transform=transform))\n",
        "\n",
        "  hyper_parameters = {\n",
        "      'model': model,\n",
        "      'lr': 1e-3,\n",
        "      'bts': 1,\n",
        "      'dataset': dataset,\n",
        "      'epochs': 20,\n",
        "      'name': name\n",
        "  }\n",
        "  train(hyper_parameters)\n",
        "\n",
        "\n",
        "def exp_pca(ftrs):\n",
        "  return batch_PCA(torch.exp(ftrs))"
      ],
      "metadata": {
        "id": "DGNsd8h-TBtE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Networks"
      ],
      "metadata": {
        "id": "LdmB6u0loBwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data stats"
      ],
      "metadata": {
        "id": "3aaFm_f2l3k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class rgb2hsv():\n",
        "    \"\"\"Convert images from RGB to HSV.\"\"\"\n",
        "    def __call__(self, image):\n",
        "        # swap color axis because\n",
        "        # torch image: C x H x W\n",
        "        # numpy image: H x W x C\n",
        "        image = image.cpu().numpy()\n",
        "        image = image.transpose((1, 2, 0))\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
        "        hsv = hsv.transpose((2, 0, 1))\n",
        "        hsv = torch.from_numpy(hsv)\n",
        "        return hsv\n",
        "\n",
        "\n",
        "def patch_stat(ds):\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=False)\n",
        "  hist = torch.zeros(3,256)\n",
        "  for im, _ in dl:\n",
        "    for i in range(256):\n",
        "      for c in range(3):\n",
        "        hist[c][i] += torch.sum(im[0][c] == i)\n",
        "  norm = torch.sum(hist[0][:])\n",
        "  hist = hist / norm\n",
        "  plt.plot(range(256), hist[0][:], color='r')\n",
        "  plt.plot(range(256), hist[1][:], color='g')\n",
        "  plt.plot(range(256), hist[2][:], color='b')\n",
        "  #plt.show()\n",
        "  hist /= 3\n",
        "  ent = (-hist) * torch.log(hist + 0.0001)\n",
        "  ent = torch.sum(ent)\n",
        "  print(\"Data entropy is : {}\".format(ent))\n",
        "\n"
      ],
      "metadata": {
        "id": "OeJEgOYgMAhQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "ds_hsv = PatchDataset('./../dataset_ds', transform=rgb2hsv())\n",
        "plt.subplot(321)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - all\")\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSI\")\n",
        "plt.subplot(323)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSI\")\n",
        "ds_hsv.reset_filter()\n",
        "\n",
        "ds_hsv.set_filter(lb=\"MSS\")\n",
        "plt.subplot(325)\n",
        "patch_stat(ds_hsv)\n",
        "plt.title(\"HSV - MSS\")\n",
        "\n",
        "\n",
        "ds_rgb = PatchDataset('./../dataset_ds')\n",
        "plt.subplot(322)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - all\")\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSI\")\n",
        "plt.subplot(324)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSI\")\n",
        "ds_rgb.reset_filter()\n",
        "\n",
        "ds_rgb.set_filter(lb=\"MSS\")\n",
        "plt.subplot(326)\n",
        "patch_stat(ds_rgb)\n",
        "plt.title(\"RGB - MSS\")"
      ],
      "metadata": {
        "id": "VkN0MKOcX3yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_res(dir):\n",
        "  ds = torchvision.datasets.DatasetFolder(root=dir, loader=read_mtx, extensions='.npy')\n",
        "  dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=True)\n",
        "\n",
        "  for i, data in enumerate(dl):\n",
        "    if i < 1:\n",
        "      mat, lb = data\n",
        "      lb= int(lb.cpu())\n",
        "      mat = mat.numpy()[0]\n",
        "      ln = len(mat[:,0])\n",
        "      cl = 0 if lb == 0 else 90\n",
        "      cl = [cl + int(10*l/ln) for l in range(ln)]\n",
        "      print(cl)\n",
        "      plt.scatter(mat[:,0], mat[:,1], c=cl)\n",
        "  plt.show()\n",
        "  \n",
        "show_res('../dataset_2d_matrix_mm')"
      ],
      "metadata": {
        "id": "0XOtJPuHyN0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## runs"
      ],
      "metadata": {
        "id": "1xQLeTwFl60C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_2d_res', incpt3_out2, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "U6uLbdNpMFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trf = transforms.Compose([transforms.ToPILImage(),\n",
        "                          transforms.ToTensor(),\n",
        "                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
        "patient_data('./../dataset', './../dataset_100d_res', incpt3_out100, patientRes, trf=trf)"
      ],
      "metadata": {
        "id": "5bjNG4Y1BTmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_2d_res/train', '../dataset_2d_res/test', MLPmodel(8, batch_PCA(k=2, mean=True, eig_val=True, fnc=torch.exp)), \"LR_test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gWPSlWJgAljl",
        "outputId": "3a51f285-aba4-4a96-b618-316851fc6c4e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260\n",
            "{'MSIMUT': 0, 'MSS': 1}\n",
            "100\n",
            "{'MSIMUT': 0, 'MSS': 1}\n",
            "Epoch:  1 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 43.05 secs\n",
            "Epoch:  2 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.98 secs\n",
            "Epoch:  3 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.00 secs\n",
            "Epoch:  4 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.03 secs\n",
            "Epoch:  5 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.98 secs\n",
            "Epoch:  6 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.98 secs\n",
            "Epoch:  7 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.97 secs\n",
            "Epoch:  8 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.99 secs\n",
            "Epoch:  9 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.03 secs\n",
            "Epoch: 10 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.07 secs\n",
            "Epoch: 11 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.99 secs\n",
            "Epoch: 12 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.99 secs\n",
            "Epoch: 13 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.99 secs\n",
            "Epoch: 14 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.97 secs\n",
            "Epoch: 15 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 1.99 secs\n",
            "Epoch: 16 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.00 secs\n",
            "Epoch: 17 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.02 secs\n",
            "Epoch: 18 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.06 secs\n",
            "Epoch: 19 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.03 secs\n",
            "Epoch: 20 | Loss: nan | Training accuracy: 50.000% | Test accuracy: 50.000% | Epoch Time: 2.02 secs\n",
            "==> Finished Training ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-2c9ddcf2e606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../dataset_2d_res/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../dataset_2d_res/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLPmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_PCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfnc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LR_test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-33e7bcd849f5>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(dir_train, dir_test, model, name)\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   }\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyper_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-35d70c8c8132>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mylim\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mset_ylim\u001b[0;34m(self, bottom, top, emit, auto, ymin, ymax)\u001b[0m\n\u001b[1;32m   3576\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_unit_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mydata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3577\u001b[0m         \u001b[0mbottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_converted_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3578\u001b[0;31m         \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_converted_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbottom\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_validate_converted_limits\u001b[0;34m(self, limit, convert)\u001b[0m\n\u001b[1;32m   3099\u001b[0m             if (isinstance(converted_limit, Real)\n\u001b[1;32m   3100\u001b[0m                     and not np.isfinite(converted_limit)):\n\u001b[0;32m-> 3101\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Axis limits cannot be NaN or Inf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3102\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Axis limits cannot be NaN or Inf"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_100d_res/train', '../dataset_100d_res/test', LinearReg(200, exp_pca), \"100d_pca\")"
      ],
      "metadata": {
        "id": "zQgkqMQmmR5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process('../dataset_100d_res/train', '../dataset_100d_res/test', MLPmodel(200, exp_pca), \"100d_pca\")"
      ],
      "metadata": {
        "id": "9VHqTfgW2CjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grave yard"
      ],
      "metadata": {
        "id": "xg44H4J-Um71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class vgg16_features(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(vgg16_features, self).__init__()\n",
        "    self.m = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "  def forward(self, imgs):\n",
        "    ftrs = self.m.features(imgs)\n",
        "    return torch.max(ftrs, dim=1).values\n",
        "    "
      ],
      "metadata": {
        "id": "La5H9qOey8Vs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, head, rest):\n",
        "    super(AutoEncoder, self).__init__()\n",
        "    self.encode = head\n",
        "    self.decode = rest\n",
        "  \n",
        "  def forward(self, imgs):\n",
        "    x = self.encode(imgs)\n",
        "    return self.decode(x)\n",
        "\n",
        "  def feature_ext(self, imgs):\n",
        "    return self.encode(imgs)"
      ],
      "metadata": {
        "id": "8ReMGN_XAxsF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class patches_DataSet(torchvision.datasets.DatasetFolder):\n",
        "  def __init__(self, root='./data'):\n",
        "    super(torchvision.datasets.DatasetFolder, self).init__(root=root, loader=torchvision.io.read_image, extensions='.png')\n",
        "\n",
        "  def find_classes(dir):\n",
        "    return None"
      ],
      "metadata": {
        "id": "5aqZoNEuQTRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(ds, dir):\n",
        "  cls = ds.find_classes(dir)[0]\n",
        "  ret = []\n",
        "  for c in cls:\n",
        "    case = c[-1]\n",
        "    ret.append(case)\n",
        "  return ret"
      ],
      "metadata": {
        "id": "zLBpwMzK0AC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Project_jupyter.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}